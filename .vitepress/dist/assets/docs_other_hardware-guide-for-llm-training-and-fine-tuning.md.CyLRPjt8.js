import{_ as a,c as i,o as e,ag as t}from"./chunks/framework.ByciF0Oj.js";const d=JSON.parse('{"title":"大模型训练/微调硬件指南","description":"","frontmatter":{},"headers":[],"relativePath":"docs/other/hardware-guide-for-llm-training-and-fine-tuning.md","filePath":"docs/other/hardware-guide-for-llm-training-and-fine-tuning.md","lastUpdated":1740531902000}'),n={name:"docs/other/hardware-guide-for-llm-training-and-fine-tuning.md"};function o(r,l,p,h,u,c){return e(),i("div",null,l[0]||(l[0]=[t('<h1 id="大模型训练-微调硬件指南" tabindex="-1">大模型训练/微调硬件指南 <a class="header-anchor" href="#大模型训练-微调硬件指南" aria-label="Permalink to &quot;大模型训练/微调硬件指南&quot;">​</a></h1><p>在本综合指南中，我们深入研究了训练和微调 LLM 所需的基本硬件设置，从普通的 7B/8B 模型到尖端的 70B 模型，以帮助你实现 AI 抱负。</p><h3 id="关于langchat" tabindex="-1">关于LangChat <a class="header-anchor" href="#关于langchat" aria-label="Permalink to &quot;关于LangChat&quot;">​</a></h3><p><strong>LangChat</strong> 是Java生态下企业级AIGC项目解决方案，集成RBAC和AIGC大模型能力，帮助企业快速定制AI知识库、企业AI机器人。</p><p><strong>支持的AI大模型：</strong> Gitee AI / 阿里通义 / 百度千帆 / DeepSeek / 抖音豆包 / 智谱清言 / 零一万物 / 讯飞星火 / OpenAI / Gemini / Ollama / Azure / Claude 等大模型。</p><ul><li>官网地址：<a href="http://langchat.cn/" target="_blank" rel="noreferrer">http://langchat.cn/</a></li></ul><p><strong>开源地址：</strong></p><ul><li>Gitee：<a href="https://gitee.com/langchat/langchat" target="_blank" rel="noreferrer">https://gitee.com/langchat/langchat</a></li><li>Github：<a href="https://github.com/tycoding/langchat" target="_blank" rel="noreferrer">https://github.com/tycoding/langchat</a></li></ul><p><img src="http://cdn.tycoding.cn/docs/202502151026673.png" alt="iShot_2025-02-12_12.18.53" loading="lazy"></p><p>人工智能的快速发展导致了大型语言模型 (LLM) 的出现，这些模型能够解决复杂的任务并推动整个行业的创新。</p><p>然而，训练和微调这些模型需要大量的计算能力。无论你是人工智能爱好者、研究人员还是数据科学家，了解 LLM 的硬件要求对于优化性能和成本效益都至关重要。</p><p>在本综合指南中，我们深入研究了训练和微调 LLM 所需的基本硬件设置，从普通的 7B/8B 模型到尖端的 70B 模型，以帮助你实现 AI 抱负。</p><h2 id="_1、7b-8b-模型的训练资源估计" tabindex="-1">1、7B/8B 模型的训练资源估计 <a class="header-anchor" href="#_1、7b-8b-模型的训练资源估计" aria-label="Permalink to &quot;1、7B/8B 模型的训练资源估计&quot;">​</a></h2><blockquote><p>模型大小：</p></blockquote><p>参数数量：~70 亿。</p><p>内存使用量：</p><ul><li>全精度 (FP32)：~28GB</li><li>混合精度 (FP16)：~14GB。</li></ul><blockquote><p>硬件要求：</p></blockquote><p>GPU 内存：</p><ul><li>最低设置：4 个 GPU，每个配备 16GB VRAM（例如 NVIDIA RTX 3090、4090 或 A100 40GB）。</li><li>理想设置：2–4 个 A100 GPU（每个 40GB），用于更快的训练和更大的批量大小。</li></ul><p>计算时间：</p><ul><li>示例：在 1 万亿个 token 上进行训练：在 8 个 A100 GPU（每个 40GB）上约 1 个月。</li></ul><p>存储：</p><ul><li>数据集：文本数据约 1–5TB。</li><li>检查点：保存中间状态约 500GB。</li><li>RAM：至少 128GB 用于预处理和训练支持。</li><li>网络：分布式设置的高速连接（10Gbps 或更高）。</li></ul><blockquote><p>成本估算：</p></blockquote><p>云设置：</p><ul><li>实例：4x A100 GPU。</li><li>成本：约 5 至 8 美元/小时。</li><li>总计：约 15,000 至 30,000 美元/1 万亿个 token。</li></ul><h2 id="_2、70b-模型的训练资源估算" tabindex="-1">2、70B 模型的训练资源估算 <a class="header-anchor" href="#_2、70b-模型的训练资源估算" aria-label="Permalink to &quot;2、70B 模型的训练资源估算&quot;">​</a></h2><blockquote><p>模型大小：</p></blockquote><p>参数数量：约 700 亿。</p><p>内存使用情况：</p><ul><li>全精度 (FP32)：约 280GB。</li><li>混合精度 (FP16)：约 140GB。</li></ul><blockquote><p>硬件要求：</p></blockquote><p>GPU 内存：</p><ul><li>最低设置：16 个 GPU，每个 GPU 具有 40GB VRAM（例如，NVIDIA A100 40GB）。</li><li>理想设置：32 个 A100 GPU（每个 40GB），以实现高效训练。</li></ul><p>计算时间：</p><p>示例：对 1 万亿个 token 进行训练：</p><ul><li>在 16 个 A100 GPU（每个 40GB）上进行约 2-3 个月。</li><li>在 32 个 A100 GPU 上进行约 1 个月。</li></ul><blockquote><p>存储：</p></blockquote><ul><li>数据集：大型文本数据约 10–20TB。</li><li>检查点：中间状态约 2TB 或更多。</li><li>RAM：至少 256GB；512GB 是理想的。</li><li>网络：高速互连，如 NVIDIA NVLink 或 Infiniband。</li></ul><blockquote><p>成本估算：</p></blockquote><p>云设置：</p><ul><li>实例：16x A100 GPU。</li><li>成本：约 35-50 美元/小时。</li><li>总计：1 万亿个 token 约 500,000-1,000,000 美元。</li></ul><h2 id="_3、70b-模型的微调硬件设置" tabindex="-1">3、70B 模型的微调硬件设置 <a class="header-anchor" href="#_3、70b-模型的微调硬件设置" aria-label="Permalink to &quot;3、70B 模型的微调硬件设置&quot;">​</a></h2><p>模型内存使用情况：</p><ul><li>FP32 精度：280GB。</li><li>FP16 精度：140GB。</li><li>8 位量化：70GB。</li></ul><p>硬件要求：</p><ul><li>GPU：NVIDIA A100（40GB/80GB）、H100 或多个带 NVLink 的 RTX 3090/4090 GPU。至少 8 个带 40GB VRAM 的 GPU 或 4 个带 80GB VRAM 的 GPU。</li><li>CPU：用于数据预处理的高核数 CPU（例如 AMD Threadripper 或 Intel Xeon）。</li><li>RAM：至少 256GB，用于处理大型数据集和模型卸载。</li><li>存储：至少 8TB NVMe SSD，用于数据集存储和模型检查点。</li><li>网络：用于多节点设置的高速网络（10Gbps+）。</li></ul><p>推荐的云设置：</p><p>使用 AWS、Azure 或 Google Cloud 等云提供商访问 A100/H100 GPU。</p><p>示例：</p><ul><li>AWS EC2：带有 8x A100 GPU 的 P4d 或 P5 实例。</li><li>Google Cloud：A2 Mega GPU 实例。</li></ul><h2 id="_4、7b-8b-模型的硬件要求" tabindex="-1">4、7B/8B 模型的硬件要求 <a class="header-anchor" href="#_4、7b-8b-模型的硬件要求" aria-label="Permalink to &quot;4、7B/8B 模型的硬件要求&quot;">​</a></h2><blockquote><p>内存使用情况：</p></blockquote><ul><li>16 位精度 (FP16)：~16GB VRAM。</li><li>8 位或 4 位量化：~8GB VRAM。</li></ul><blockquote><p>硬件要求：</p></blockquote><p>GPU：</p><ul><li>单 GPU 设置：NVIDIA RTX 3090/4090（24GB VRAM）。或NVIDIA A5000/A6000（24GB–48GB VRAM）。或双 GPU 设置（用于更大的批量或更快的训练）</li><li>带有 NVLink 或多 GPU 的 NVIDIA RTX 3080 Ti、3090 或 4090。</li></ul><p>预算 GPU（带量化或卸载）：</p><ul><li>RTX 3060（12GB VRAM）、RTX 3070 Ti（8GB VRAM）。</li><li>CPU：用于数据预处理和后台任务的多核 CPU。— 推荐：AMD Ryzen 7/9、Intel Core i7/i9。</li><li>RAM：— 最低：32GB（适用于带量化的轻量级工作负载）。— 推荐：64GB 或更多，适用于较大的数据集或 CPU 卸载。</li><li>存储：— 使用 NVMe SSD 进行快速读写操作。— 至少 1TB 用于数据集、模型检查点和日志。— 对于较大的数据集：2TB 或更多。</li><li>电源：确保 GPU 有足够的功率：— 单 GPU：750W PSU。— 双 GPU：1000W PSU。</li><li>网络（如果是分布式的）：对于多节点训练：10Gbps 或更高的以太网连接。</li></ul><h2 id="_5、关键见解和行业实践" tabindex="-1">5、关键见解和行业实践 <a class="header-anchor" href="#_5、关键见解和行业实践" aria-label="Permalink to &quot;5、关键见解和行业实践&quot;">​</a></h2><ul><li>数据规模：根据 Common Crawl，2023 年 6 月，网络抓取包含约 30 亿个网页和约 400TB 的未压缩数据，突显了高质量 LLM 培训所需的庞大数据集。</li><li>云与本地：云解决方案提供了灵活性和可扩展性，但对于需要频繁进行 LLM 训练和微调的组织来说，本地设置可能更具成本效益。</li><li>精度权衡：量化技术（8 位或 4 位）显著降低了内存需求，使较小的设置也可以进行微调。</li></ul><h2 id="_6、结束语" tabindex="-1">6、结束语 <a class="header-anchor" href="#_6、结束语" aria-label="Permalink to &quot;6、结束语&quot;">​</a></h2><p>训练和微调 LLM 需要大量的计算资源，但 GPU 技术、云服务和精度方面的进步、视觉优化使这些任务变得更加可行。</p><p>无论是从头开始构建模型还是定制预先训练的模型，了解硬件要求对于成功部署都至关重要。平衡成本、效率和可扩展性将确保你的 LLM 工作流程既实用又有效。</p><h2 id="联系我" tabindex="-1">联系我 <a class="header-anchor" href="#联系我" aria-label="Permalink to &quot;联系我&quot;">​</a></h2><p>最后，推荐大家关注一下开源项目：LangChat，Java生态下的AIGC大模型产品解决方案。</p><ul><li>LangChat产品官网：<a href="https://langchat.cn/" target="_blank" rel="noreferrer">https://langchat.cn/</a></li><li>Github: <a href="https://github.com/TyCoding/langchat" target="_blank" rel="noreferrer">https://github.com/TyCoding/langchat</a></li><li>Gitee: <a href="https://gitee.com/langchat/langchat" target="_blank" rel="noreferrer">https://gitee.com/langchat/langchat</a></li><li>微信：LangchainChat</li></ul><p><img src="http://cdn.tycoding.cn/docs/202502151026673.png" alt="" loading="lazy"></p>',69)]))}const g=a(n,[["render",o]]);export{d as __pageData,g as default};
