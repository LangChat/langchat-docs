import{_ as l,c as t,o as e,ag as i}from"./chunks/framework.ByciF0Oj.js";const g=JSON.parse('{"title":"VLLM vs. Ollama","description":"","frontmatter":{},"headers":[],"relativePath":"docs/other/vllm-vs-ollama.md","filePath":"docs/other/vllm-vs-ollama.md","lastUpdated":1740531902000}'),n={name:"docs/other/vllm-vs-ollama.md"};function s(o,a,p,c,r,h){return e(),t("div",null,a[0]||(a[0]=[i(`<h1 id="vllm-vs-ollama" tabindex="-1">VLLM vs. Ollama <a class="header-anchor" href="#vllm-vs-ollama" aria-label="Permalink to &quot;VLLM vs. Ollama&quot;">​</a></h1><blockquote><p>大型语言模型 (LLM) 的兴起改变了 AI 驱动的应用程序，开发人员依赖于优化的推理框架，这个领域的两个杰出解决方案是 VLLM 和 Ollama。</p></blockquote><p><img src="http://www.hubwiz.com/blog/content/images/size/w2000/2025/01/vllm-vs-ollama.png" alt="VLLM vs. Ollama" loading="lazy"></p><h3 id="关于langchat" tabindex="-1">关于LangChat <a class="header-anchor" href="#关于langchat" aria-label="Permalink to &quot;关于LangChat&quot;">​</a></h3><p><strong>LangChat</strong> 是Java生态下企业级AIGC项目解决方案，集成RBAC和AIGC大模型能力，帮助企业快速定制AI知识库、企业AI机器人。</p><p><strong>支持的AI大模型：</strong> Gitee AI / 阿里通义 / 百度千帆 / DeepSeek / 抖音豆包 / 智谱清言 / 零一万物 / 讯飞星火 / OpenAI / Gemini / Ollama / Azure / Claude 等大模型。</p><ul><li>官网地址：<a href="http://langchat.cn/" target="_blank" rel="noreferrer">http://langchat.cn/</a></li></ul><p><strong>开源地址：</strong></p><ul><li>Gitee：<a href="https://gitee.com/langchat/langchat" target="_blank" rel="noreferrer">https://gitee.com/langchat/langchat</a></li><li>Github：<a href="https://github.com/tycoding/langchat" target="_blank" rel="noreferrer">https://github.com/tycoding/langchat</a></li></ul><p><img src="http://cdn.tycoding.cn/docs/202502151026673.png" alt="iShot_2025-02-12_12.18.53" loading="lazy"></p><p>大型语言模型 (LLM) 的兴起改变了 AI 驱动的应用程序，实现了从聊天机器人到自动代码生成的一切。然而，高效运行这些模型仍然是一个挑战，因为它们通常需要大量的计算资源。</p><p>为了解决这个问题，开发人员依赖于优化的推理框架，旨在最大限度地提高速度、最大限度地减少内存使用量并无缝集成到应用程序中。这个领域的两个杰出解决方案是 VLLM 和 Ollama——每个解决方案都满足不同的需求。</p><ul><li>VLLM 是一个优化的推理引擎，可提供高速令牌生成和高效的内存管理，使其成为大型 AI 应用程序的理想选择。</li><li>Ollama 是一个轻量级且用户友好的框架，可简化在本地机器上运行开源 LLM 的过程。</li></ul><p>那么，你应该选择哪一个呢？在这次全面的比较中，我们将分解它们的性能、易用性、用例、替代方案和分步设置，以帮助你做出明智的决定。</p><p><img src="http://www.hubwiz.com/blog/content/images/2025/01/image-509.png" alt="img" loading="lazy"></p><h2 id="_1、vllm-和-ollama概述" tabindex="-1">1、VLLM 和 Ollama概述 <a class="header-anchor" href="#_1、vllm-和-ollama概述" aria-label="Permalink to &quot;1、VLLM 和 Ollama概述&quot;">​</a></h2><p>在深入了解细节之前，让我们先了解这两个框架的核心目的。</p><p>VLLM（超大型语言模型）是由 SKYPILOT 构建的推理优化框架，旨在提高在 GPU 上运行的 LLM 的效率。它专注于：</p><ul><li>使用连续批处理快速生成令牌。</li><li>通过 PagedAttention 实现高效的内存使用，允许处理大型上下文窗口而不会消耗过多的 GPU 内存。</li><li>无缝集成到 AI 工作流中，兼容 PyTorch 和 TensorFlow 等主要深度学习平台。</li></ul><p>VLLM 被需要大规模高性能推理的 AI 研究人员和企业广泛使用。</p><p>Ollama 是一个本地 LLM 运行时，可简化部署和使用开源 AI 模型。它提供：</p><ul><li>预打包模型，例如 LLaMA、Mistral 和 Falcon。</li><li>优化的 CPU 和 GPU 推理，用于在日常硬件上运行 AI 模型。</li><li>一个简单的 API 和 CLI，允许开发人员以最少的配置启动 LLM。</li></ul><p>对于希望在个人机器上试验 AI 模型的开发人员和 AI 爱好者来说，Ollama 是一个绝佳的选择。</p><h2 id="_2、性能-速度、内存和可扩展性" tabindex="-1">2、性能：速度、内存和可扩展性 <a class="header-anchor" href="#_2、性能-速度、内存和可扩展性" aria-label="Permalink to &quot;2、性能：速度、内存和可扩展性&quot;">​</a></h2><p>性能是选择推理框架的关键因素。让我们在速度、内存效率和可扩展性方面比较一下 VLLM 和 Ollama。</p><p>关键性能指标：</p><p><img src="http://www.hubwiz.com/blog/content/images/2025/01/image-510.png" alt="img" loading="lazy"></p><p>VLLM 利用 PagedAttention 来最大化推理速度并有效处理大型上下文窗口。这使得它成为聊天机器人、搜索引擎和 AI 写作助手等高性能 AI 应用程序的首选解决方案。</p><p>Ollama 提供了不错的速度，但受到本地硬件的限制。它非常适合在 MacBook、PC 和边缘设备上运行较小的模型，但在处理非常大的模型时会遇到困难。</p><blockquote><p>结论：Ollama 更适合初学者，而 VLLM 是需要深度定制的开发人员的选择。</p></blockquote><h2 id="_3、用例-何时使用-vllm-而不是-ollama" tabindex="-1">3、用例：何时使用 VLLM 而不是 Ollama？ <a class="header-anchor" href="#_3、用例-何时使用-vllm-而不是-ollama" aria-label="Permalink to &quot;3、用例：何时使用 VLLM 而不是 Ollama？&quot;">​</a></h2><p>VLLM 的最佳用例</p><ul><li>企业 AI 应用程序（例如客户服务机器人、AI 驱动的搜索引擎）</li><li>在高端 GPU（A100、H100、RTX 4090 等）上部署基于云的 LLM</li><li>微调和运行自定义模型</li><li>需要大型上下文窗口的应用程序</li></ul><p>不适合：个人笔记本电脑、休闲 AI 实验</p><p>Ollama 的最佳用例</p><ul><li>在没有云资源的情况下在 Mac、Windows 或 Linux 上运行 LLM</li><li>无需复杂设置即可在本地试验模型</li><li>想要使用简单 API 将 AI 集成到应用程序中的开发人员</li><li>边缘计算应用程序</li></ul><p>不适合：大规模 AI 部署、繁重的 GPU 工作负载</p><blockquote><p>结论：VLLM 适用于 AI 工程师，而 Ollama 适用于开发人员和业余爱好者。</p></blockquote><h2 id="_4、快速上手" tabindex="-1">4、快速上手 <a class="header-anchor" href="#_4、快速上手" aria-label="Permalink to &quot;4、快速上手&quot;">​</a></h2><p>VLLM要首先安装依赖项：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>pip install vllm</span></span></code></pre></div><p>在 LLaMA 模型上运行推理：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>from vllm import LLM</span></span>
<span class="line"><span>llm = LLM(model=&quot;meta-llama/Llama-2-7b&quot;)</span></span>
<span class="line"><span>output = llm.generate(&quot;What is VLLM?&quot;)</span></span></code></pre></div><p>Ollama要安装 Ollama (Mac/Linux)：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>brew install ollama</span></span></code></pre></div><p>然后下载并运行模型：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>ollama run mistral</span></span></code></pre></div><p>调用 Ollama 的 API：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>import requests</span></span>
<span class="line"><span>response = requests.post(&quot;http://localhost:11434/api/generate&quot;, json={&quot;model&quot;: &quot;mistral&quot;, &quot;prompt&quot;: &quot;Tell me a joke&quot;})</span></span>
<span class="line"><span>print(response.json())</span></span></code></pre></div><blockquote><p>结论：Ollama 更易于安装，而VLLM 提供更多定制。</p></blockquote><h2 id="联系我" tabindex="-1">联系我 <a class="header-anchor" href="#联系我" aria-label="Permalink to &quot;联系我&quot;">​</a></h2><p>最后，推荐大家关注一下开源项目：LangChat，Java生态下的AIGC大模型产品解决方案。</p><ul><li>LangChat产品官网：<a href="https://langchat.cn/" target="_blank" rel="noreferrer">https://langchat.cn/</a></li><li>Github: <a href="https://github.com/TyCoding/langchat" target="_blank" rel="noreferrer">https://github.com/TyCoding/langchat</a></li><li>Gitee: <a href="https://gitee.com/langchat/langchat" target="_blank" rel="noreferrer">https://gitee.com/langchat/langchat</a></li><li>微信：LangchainChat</li></ul><p><img src="http://cdn.tycoding.cn/docs/202502151026673.png" alt="iShot_2025-02-12_12.18.53" loading="lazy"></p>`,54)]))}const d=l(n,[["render",s]]);export{g as __pageData,d as default};
