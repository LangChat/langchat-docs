import{_ as e,c as n,o as s,ag as t}from"./chunks/framework.ByciF0Oj.js";const u=JSON.parse('{"title":"DeepSeek-R1微调指南","description":"","frontmatter":{},"headers":[],"relativePath":"docs/other/deepseek-r1-tuning.md","filePath":"docs/other/deepseek-r1-tuning.md","lastUpdated":1740531902000}'),p={name:"docs/other/deepseek-r1-tuning.md"};function l(i,a,o,r,c,h){return s(),n("div",null,a[0]||(a[0]=[t(`<h1 id="deepseek-r1微调指南" tabindex="-1">DeepSeek-R1微调指南 <a class="header-anchor" href="#deepseek-r1微调指南" aria-label="Permalink to &quot;DeepSeek-R1微调指南&quot;">​</a></h1><blockquote><p>在这篇博文中，我们将逐步指导你在消费级 GPU 上使用 LoRA（低秩自适应）和 Unsloth 对 DeepSeek-R1 进行微调。</p></blockquote><h3 id="关于langchat" tabindex="-1">关于LangChat <a class="header-anchor" href="#关于langchat" aria-label="Permalink to &quot;关于LangChat&quot;">​</a></h3><p><strong>LangChat</strong> 是Java生态下企业级AIGC项目解决方案，集成RBAC和AIGC大模型能力，帮助企业快速定制AI知识库、企业AI机器人。</p><p><strong>支持的AI大模型：</strong> Gitee AI / 阿里通义 / 百度千帆 / DeepSeek / 抖音豆包 / 智谱清言 / 零一万物 / 讯飞星火 / OpenAI / Gemini / Ollama / Azure / Claude 等大模型。</p><ul><li>官网地址：<a href="http://langchat.cn/" target="_blank" rel="noreferrer">http://langchat.cn/</a></li></ul><p><strong>开源地址：</strong></p><ul><li>Gitee：<a href="https://gitee.com/langchat/langchat" target="_blank" rel="noreferrer">https://gitee.com/langchat/langchat</a></li><li>Github：<a href="https://github.com/tycoding/langchat" target="_blank" rel="noreferrer">https://github.com/tycoding/langchat</a></li></ul><p><img src="http://cdn.tycoding.cn/docs/202502151026673.png" alt="iShot_2025-02-12_12.18.53" loading="lazy"></p><p><img src="http://www.hubwiz.com/blog/content/images/size/w2000/2025/02/deepseek-r1-fine-tuning-guide.png" alt="DeepSeek-R1微调指南" loading="lazy"></p><p>微调像 DeepSeek-R1 这样的大型 AI 模型可能需要大量资源，但使用正确的工具，可以在消费级硬件上进行有效训练。让我们探索如何使用 LoRA（低秩自适应）和 Unsloth 优化 DeepSeek-R1 微调，从而实现更快、更具成本效益的训练。</p><p>DeepSeek 的最新 R1 模型正在设定推理性能的新基准，可与专有模型相媲美，同时保持开源。 DeepSeek-R1 的精简版本在 Llama 3 和 Qwen 2.5 上进行了训练，现在已针对使用 Unsloth（一种专为高效模型自适应而设计的框架）进行微调进行了高度优化。⚙</p><p>在这篇博文中，我们将逐步指导你在消费级 GPU 上使用 LoRA（低秩自适应）和 Unsloth 对 DeepSeek-R1 进行微调。</p><h2 id="_1、了解-deepseek-r1" tabindex="-1">1、了解 DeepSeek-R1 <a class="header-anchor" href="#_1、了解-deepseek-r1" aria-label="Permalink to &quot;1、了解 DeepSeek-R1&quot;">​</a></h2><p>DeepSeek-R1 是由 DeepSeek 开发的开源推理模型。它在需要逻辑推理、数学问题解决和实时决策的任务中表现出色。与传统 LLM 不同，DeepSeek-R1 的推理过程透明，适合……</p><h3 id="_1-1-为什么需要微调" tabindex="-1">1.1 为什么需要微调？ <a class="header-anchor" href="#_1-1-为什么需要微调" aria-label="Permalink to &quot;1.1 为什么需要微调？&quot;">​</a></h3><p>微调是将 DeepSeek-R1 等通用语言模型适应特定任务、行业或数据集的关键步骤。</p><p>微调之所以重要，原因如下：</p><ul><li>领域特定知识：预训练模型是在大量通用知识库上进行训练的。微调允许针对医疗保健、金融或法律分析等特定领域进行专业化。</li><li>提高准确性：自定义数据集可帮助模型理解小众术语、结构和措辞，从而获得更准确的响应。</li><li>任务适应：微调使模型能够更高效地执行聊天机器人交互、文档摘要或问答等任务。</li><li>减少偏差：根据特定数据集调整模型权重有助于减轻原始训练数据中可能存在的偏差。</li></ul><p>通过微调 DeepSeek-R1，开发人员可以根据其特定用例对其进行定制，从而提高其有效性和可靠性。</p><h3 id="_1-2-微调中的常见挑战及其克服方法" tabindex="-1">1.2 微调中的常见挑战及其克服方法 <a class="header-anchor" href="#_1-2-微调中的常见挑战及其克服方法" aria-label="Permalink to &quot;1.2 微调中的常见挑战及其克服方法&quot;">​</a></h3><p>微调大规模 AI 模型面临多项挑战。以下是一些最常见的挑战及其解决方案：</p><p>a) 计算限制</p><ul><li>挑战：微调 LLM 需要具有大量 VRAM 和内存资源的高端 GPU。</li><li>解决方案：使用 LoRA 和 4 位量化来减少计算负荷。将某些进程卸载到 CPU 或基于云的服务（如 Google Colab 或 AWS）也会有所帮助。</li></ul><p>b) 在小型数据集上过度拟合</p><ul><li>挑战：在小型数据集上进行训练可能会导致模型记住响应，而不是很好地进行泛化。</li><li>解决方案：使用数据增强技术和正则化方法（如 dropout 或 early stopping）来防止过度拟合。</li></ul><p>c) 训练时间长</p><ul><li>挑战：微调可能需要几天或几周的时间，具体取决于硬件和数据集大小。</li><li>解决方案：利用梯度检查点和低秩自适应 (LoRA) 来加快训练速度，同时保持效率。</li></ul><p>d) 灾难性遗忘</p><ul><li>挑战：微调后的模型可能会忘记预训练阶段的一般知识。</li><li>解决方案：使用包含特定领域数据和一般知识数据的混合数据集来保持整体模型准确性。</li></ul><p>e) 微调模型中的偏差</p><ul><li>挑战：微调模型可以继承数据集中存在的偏差。</li><li>解决方案：整理多样化且无偏差的数据集，应用去偏差技术并使用公平性指标评估模型。</li></ul><p>有效应对这些挑战可确保稳健高效的微调过程。</p><h2 id="_2、设置环境" tabindex="-1">2、设置环境 <a class="header-anchor" href="#_2、设置环境" aria-label="Permalink to &quot;2、设置环境&quot;">​</a></h2><p>微调大型语言模型 (LLM) 需要大量计算资源。以下是推荐的配置：</p><p><img src="http://www.hubwiz.com/blog/content/images/2025/02/image-27.png" alt="img" loading="lazy"></p><p>确保你拥有 Python 3.8+ 并安装必要的依赖项：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>pip install unsloth torch transformers datasets accelerate bitsandbytes</span></span></code></pre></div><h2 id="_3、加载预训练模型和-tokenizer" tabindex="-1">3、加载预训练模型和 Tokenizer <a class="header-anchor" href="#_3、加载预训练模型和-tokenizer" aria-label="Permalink to &quot;3、加载预训练模型和 Tokenizer&quot;">​</a></h2><p>使用 Unsloth，我们可以高效地以 4 位量化加载模型以减少内存使用量：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>from unsloth import FastLanguageModel</span></span>
<span class="line"><span></span></span>
<span class="line"><span>model_name = &quot;unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit&quot;</span></span>
<span class="line"><span>max_seq_length = 2048</span></span>
<span class="line"><span>model, tokenizer = FastLanguageModel.from_pretrained(</span></span>
<span class="line"><span>    model_name=model_name,</span></span>
<span class="line"><span>    max_seq_length=max_seq_length,</span></span>
<span class="line"><span>    load_in_4bit=True,</span></span>
<span class="line"><span>)</span></span></code></pre></div><h2 id="_4、准备数据集" tabindex="-1">4、准备数据集 <a class="header-anchor" href="#_4、准备数据集" aria-label="Permalink to &quot;4、准备数据集&quot;">​</a></h2><p>微调需要结构化的输入输出对。让我们假设一个用于遵循指令任务的数据集：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>{&quot;instruction&quot;: &quot;What is the capital of France?&quot;, &quot;output&quot;: &quot;The capital of France is Paris.&quot;}</span></span>
<span class="line"><span>{&quot;instruction&quot;: &quot;Solve: 2 + 2&quot;, &quot;output&quot;: &quot;The answer is 4.&quot;}</span></span></code></pre></div><p>使用 Hugging Face 的数据集库加载数据集：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>from datasets import load_dataset</span></span>
<span class="line"><span></span></span>
<span class="line"><span>dataset = load_dataset(&quot;json&quot;, data_files={&quot;train&quot;: &quot;train_data.jsonl&quot;, &quot;test&quot;: &quot;test_data.jsonl&quot;})</span></span></code></pre></div><p>使用聊天式提示模板格式化数据集：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>prompt_template = &quot;&quot;&quot;Below is an instruction that describes a task. Write a response that appropriately completes the request.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>### Instruction:</span></span>
<span class="line"><span>{instruction}</span></span>
<span class="line"><span>### Response:</span></span>
<span class="line"><span>&quot;&quot;&quot;</span></span>
<span class="line"><span>def preprocess_function(examples):</span></span>
<span class="line"><span>    inputs = [prompt_template.format(instruction=inst) for inst in examples[&quot;instruction&quot;]]</span></span>
<span class="line"><span>    model_inputs = tokenizer(inputs, max_length=max_seq_length, truncation=True)</span></span>
<span class="line"><span>    return model_inputs</span></span>
<span class="line"><span>tokenized_dataset = dataset.map(preprocess_function, batched=True)</span></span></code></pre></div><h2 id="_5、应用-lora-进行高效微调" tabindex="-1">5、应用 LoRA 进行高效微调 <a class="header-anchor" href="#_5、应用-lora-进行高效微调" aria-label="Permalink to &quot;5、应用 LoRA 进行高效微调&quot;">​</a></h2><p>LoRA 允许通过仅训练模型的特定部分进行微调，从而显著减少内存使用量：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>model = FastLanguageModel.get_peft_model(</span></span>
<span class="line"><span>    model,</span></span>
<span class="line"><span>    r=16,  # LoRA rank</span></span>
<span class="line"><span>    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],  # Fine-tune key attention layers</span></span>
<span class="line"><span>    lora_alpha=32,</span></span>
<span class="line"><span>    lora_dropout=0.05,</span></span>
<span class="line"><span>    bias=&quot;none&quot;,</span></span>
<span class="line"><span>    use_gradient_checkpointing=True,</span></span>
<span class="line"><span>)</span></span></code></pre></div><p>训练模型。配置训练参数。初始化并开始训练：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>from transformers import Trainer</span></span>
<span class="line"><span></span></span>
<span class="line"><span>trainer = Trainer(</span></span>
<span class="line"><span>    model=model,</span></span>
<span class="line"><span>    args=training_args,</span></span>
<span class="line"><span>    train_dataset=tokenized_dataset[&quot;train&quot;],</span></span>
<span class="line"><span>    eval_dataset=tokenized_dataset[&quot;test&quot;],</span></span>
<span class="line"><span>    tokenizer=tokenizer,</span></span>
<span class="line"><span>)</span></span>
<span class="line"><span>trainer.train()</span></span></code></pre></div><h2 id="_6、评估和保存模型" tabindex="-1">6、评估和保存模型 <a class="header-anchor" href="#_6、评估和保存模型" aria-label="Permalink to &quot;6、评估和保存模型&quot;">​</a></h2><p>训练后，评估并保存微调后的模型：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># Evaluate the model</span></span>
<span class="line"><span>eval_results = trainer.evaluate()</span></span>
<span class="line"><span>print(f&quot;Perplexity: {eval_results[&#39;perplexity&#39;]}&quot;)</span></span>
<span class="line"><span></span></span>
<span class="line"><span># Save the model and tokenizer</span></span>
<span class="line"><span>model.save_pretrained(&quot;./finetuned_deepseek_r1&quot;)</span></span>
<span class="line"><span>tokenizer.save_pretrained(&quot;./finetuned_deepseek_r1&quot;)</span></span></code></pre></div><h2 id="_7、部署模型进行推理" tabindex="-1">7、部署模型进行推理 <a class="header-anchor" href="#_7、部署模型进行推理" aria-label="Permalink to &quot;7、部署模型进行推理&quot;">​</a></h2><p>微调后，使用模型进行推理。本地部署llama.cpp，运行：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>./llama.cpp/llama-cli \\</span></span>
<span class="line"><span>   --model unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf \\</span></span>
<span class="line"><span>   --cache-type-k q8_0 \\</span></span>
<span class="line"><span>   --threads 16 \\</span></span>
<span class="line"><span>   --prompt &#39;&lt;|User|&gt;What is 1+1?&lt;|Assistant|&gt;&#39; \\</span></span>
<span class="line"><span>   --n-gpu-layers 20 \\</span></span>
<span class="line"><span>   -no-cnv</span></span></code></pre></div><h2 id="_8、结束语" tabindex="-1">8、结束语 <a class="header-anchor" href="#_8、结束语" aria-label="Permalink to &quot;8、结束语&quot;">​</a></h2><p>通过利用 LoRA 和 Unsloth，我们成功地在消费级 GPU 上微调了 DeepSeek-R1，显著降低了内存和计算要求。这使得更快、更易于访问的 AI 模型训练成为可能，而无需昂贵的硬件。</p><h2 id="联系我" tabindex="-1">联系我 <a class="header-anchor" href="#联系我" aria-label="Permalink to &quot;联系我&quot;">​</a></h2><p>最后，推荐大家关注一下开源项目：LangChat，Java生态下的AIGC大模型产品解决方案。</p><ul><li>LangChat产品官网：<a href="https://langchat.cn/" target="_blank" rel="noreferrer">https://langchat.cn/</a></li><li>Github: <a href="https://github.com/TyCoding/langchat" target="_blank" rel="noreferrer">https://github.com/TyCoding/langchat</a></li><li>Gitee: <a href="https://gitee.com/langchat/langchat" target="_blank" rel="noreferrer">https://gitee.com/langchat/langchat</a></li><li>微信：LangchainChat</li></ul><p><img src="http://cdn.tycoding.cn/docs/202502151026673.png" alt="" loading="lazy"></p>`,65)]))}const g=e(p,[["render",l]]);export{u as __pageData,g as default};
