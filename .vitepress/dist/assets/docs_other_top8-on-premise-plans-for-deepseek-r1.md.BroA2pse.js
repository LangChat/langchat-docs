import{_ as r,c as a,o as t,ag as l}from"./chunks/framework.ByciF0Oj.js";const c=JSON.parse('{"title":"8个DeepSeek-R1私有化部署方案","description":"","frontmatter":{},"headers":[],"relativePath":"docs/other/top8-on-premise-plans-for-deepseek-r1.md","filePath":"docs/other/top8-on-premise-plans-for-deepseek-r1.md","lastUpdated":1740531902000}'),o={name:"docs/other/top8-on-premise-plans-for-deepseek-r1.md"};function i(p,e,n,s,h,g){return t(),a("div",null,e[0]||(e[0]=[l('<h1 id="_8个deepseek-r1私有化部署方案" tabindex="-1">8个DeepSeek-R1私有化部署方案 <a class="header-anchor" href="#_8个deepseek-r1私有化部署方案" aria-label="Permalink to &quot;8个DeepSeek-R1私有化部署方案&quot;">​</a></h1><blockquote><p>本文了解如何在本地和没有互联网的情况下运行DeepSeek R1推理模型，或者通过可信赖的托管服务来运行它。</p></blockquote><h3 id="关于langchat" tabindex="-1">关于LangChat <a class="header-anchor" href="#关于langchat" aria-label="Permalink to &quot;关于LangChat&quot;">​</a></h3><p><strong>LangChat</strong> 是Java生态下企业级AIGC项目解决方案，集成RBAC和AIGC大模型能力，帮助企业快速定制AI知识库、企业AI机器人。</p><p><strong>支持的AI大模型：</strong> Gitee AI / 阿里通义 / 百度千帆 / DeepSeek / 抖音豆包 / 智谱清言 / 零一万物 / 讯飞星火 / OpenAI / Gemini / Ollama / Azure / Claude 等大模型。</p><ul><li>官网地址：<a href="http://langchat.cn/" target="_blank" rel="noreferrer">http://langchat.cn/</a></li></ul><p><strong>开源地址：</strong></p><ul><li>Gitee：<a href="https://gitee.com/langchat/langchat" target="_blank" rel="noreferrer">https://gitee.com/langchat/langchat</a></li><li>Github：<a href="https://github.com/tycoding/langchat" target="_blank" rel="noreferrer">https://github.com/tycoding/langchat</a></li></ul><p><img src="http://cdn.tycoding.cn/docs/202502151026673.png" alt="iShot_2025-02-12_12.18.53" loading="lazy"></p><p><img src="https://miro.medium.com/v2/resize:fit:1400/1*Dm7EMdphvaX7BdXnUpbGCw.png" alt="img" loading="lazy"></p><p>运行DeepSeek R1 fffline并托管</p><p>许多人（尤其是开发人员）想使用新的<a href="https://api-docs.deepseek.com/news/news250120" target="_blank" rel="noreferrer">DeepSeek R1</a>思维模型，但担心将其数据发送到<a href="https://www.deepseek.com/" target="_blank" rel="noreferrer">DeepSeek</a> 。阅读本文，以了解如何在本地和不使用Internet或使用受信任的托管服务的情况下使用和运行DeepSeek R1推理模型。您将模型脱机运行，因此您的私人数据与您一起停留，并且不会将计算机留给任何LLM托管提供商（DeepSeek）。同样，借助值得信赖的托管服务，您的数据将转到第三方托管提供商而不是DeepSeek。</p><p>使用LMSTUDIO，OLLAMA和JAN在本地/离线运行DeepSeek R1或通过LLM服务平台，例如Groq，Fireworks AI以及AI共同有助于消除数据共享和隐私问题。</p><h1 id="什么是deepseek-r1" tabindex="-1">什么是DeepSeek R1？ <a class="header-anchor" href="#什么是deepseek-r1" aria-label="Permalink to &quot;什么是DeepSeek R1？&quot;">​</a></h1><p><img src="https://miro.medium.com/v2/resize:fit:1400/0*OJvhhzHtsFVTkZCu" alt="img" loading="lazy"></p><p>DeepSeek聊天UI</p><p>OpenAI O1和最新模型（例如OpenAI O3和DeepSeek R1）解决了数学，编码，科学和其他领域的复杂问题。这些模型可以考虑从用户查询的输入提示，并在生成最终解决方案之前仔细研究推理步骤或思想链（COT）。在撰写本文时，以上三种语言模型是具有思维能力的模型。 DeepSeek R1模型由基本R1模型和六个蒸馏版组成。蒸馏型从较小的版本到较大的版本，这些版本用Qwen和Llama进行了微调。</p><h1 id="人们为什么要使用r1但存在隐私问题" tabindex="-1">人们为什么要使用R1但存在隐私问题？ <a class="header-anchor" href="#人们为什么要使用r1但存在隐私问题" aria-label="Permalink to &quot;人们为什么要使用R1但存在隐私问题？&quot;">​</a></h1><p>R1模型无疑是世界上最好的推理模型之一。它的功能引起了开发人员社区对<a href="https://x.com/MatthewBerman/status/1884044269201330569" target="_blank" rel="noreferrer">X</a> ， <a href="https://www.reddit.com/r/GetNoted/comments/1ichm8v/openai_employee_gets_noted_regarding_deepseek/" target="_blank" rel="noreferrer">Reddit</a> ， <a href="https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_dont-fall-for-false-deepseek-r1-news-deepseek-activity-7289723903797600256-ox0r?utm_source=share&amp;utm_medium=member_desktop" target="_blank" rel="noreferrer">LinkedIn</a>和其他社交媒体平台的关注和关注。但是，使用DeepSeek提供的语言模型有一些错误的信息和错误的方法。例如，有些人认为DeepSeek是一个附带项目，而不是公司。其他人认为DeepSeek可能会将用户的数据用于其他目的，而不是其隐私政策中所述的内容。像OpenAI一样， <a href="https://chat.deepseek.com/" target="_blank" rel="noreferrer">DeepSeek聊天</a>的托管版本可能会收集用户的数据，并将其用于培训和改进其模型。话虽如此，这并不意味着您不应该相信使用托管的DeepSeek聊天。它与Chatgpt类似，是使用DeepSeek R1模型测试和生成响应的绝佳工具。有些人和公司不希望DeepSeek由于隐私问题而收集数据。此外，DeepSeek总部位于中国，有几个人担心与中国的一家公司共享其私人信息。</p><p>一个人如何在不与DeepSeek共享信息的情况下下载，安装和运行DeepSeek R1的思维模型家族？继续阅读以探索您和您的团队如何在没有互联网的情况下在本地运行DeepSeek R1模型，或者使用欧盟和美国基于美国的托管服务。</p><h1 id="为什么要deepseek-r1" tabindex="-1">为什么要DeepSeek R1？ <a class="header-anchor" href="#为什么要deepseek-r1" aria-label="Permalink to &quot;为什么要DeepSeek R1？&quot;">​</a></h1><p>R1模型背后的公司<a href="https://www.deepseek.com/" target="_blank" rel="noreferrer">DeepSeek</a>最近进入了主流大语言模型（LLM ）提供商，加入<a href="https://openai.com/" target="_blank" rel="noreferrer">Openai</a> ， <a href="https://ai.google.dev/" target="_blank" rel="noreferrer">Google</a> ， <a href="https://www.anthropic.com/" target="_blank" rel="noreferrer">Anthropic</a> ， <a href="https://ai.meta.com/meta-ai/" target="_blank" rel="noreferrer">Meta AI</a> ， <a href="https://groq.com/" target="_blank" rel="noreferrer">Groqinc</a> ， <a href="https://mistral.ai/" target="_blank" rel="noreferrer">Mistral</a>等主要参与者。 DeepSeek R1型号是开源的，成本低于OpenAI O1型号。成为开源为机器学习和开发人员社区提供了长期利益。人们可以为不同用例复制R1模型的版本。尽管成本较低，但它仍与OpenAI O1型号<a href="https://api-docs.deepseek.com/news/news250120" target="_blank" rel="noreferrer">相当</a>。它令人难以置信的推理功能使其成为OpenAI O1型号的绝佳选择。凭借其有趣的推理能力和低成本，许多人（包括开发人员）想使用它来为其AI应用程序供电，但担心将其数据发送到DeepSeek。的确，将DeepSeek R1模型与<a href="https://chat.deepseek.com/" target="_blank" rel="noreferrer">DeepSeek聊天</a>这样的平台使用，您的数据将由DeepSeek收集。但是，您可以在计算机上完全离线运行DeepSeek R1模型，也可以使用托管服务来运行该模型来构建您的AI应用程序。</p><p>像其他大型语言模型（LLMs ），您可以使用<a href="https://getstream.io/blog/best-local-llm-tools/" target="_blank" rel="noreferrer">本地</a>运行并测试原始的DeepSeek R1型号和机器上的DeepSeek R1家族<a href="https://getstream.io/blog/best-local-llm-tools/" target="_blank" rel="noreferrer">LLM托管工具</a>。使用R1型号的一个好处是，它们可以在诸如Groq，foring.ai等托管平台上使用。通过这些平台使用这些模型是通过DeepSeek聊天和API直接使用它们的理想选择。微软最近制作了R1型号，并在其<a href="https://azure.microsoft.com/en-us/blog/deepseek-r1-is-now-available-on-azure-ai-foundry-and-github/" target="_blank" rel="noreferrer">Azure AI Foundry</a>和Github上提供了蒸馏版。</p><h1 id="什么是当地的第一llm工具" tabindex="-1">什么是当地的第一LLM工具？ <a class="header-anchor" href="#什么是当地的第一llm工具" aria-label="Permalink to &quot;什么是当地的第一LLM工具？&quot;">​</a></h1><p>使用时LLMs像<a href="https://chatgpt.com/" target="_blank" rel="noreferrer">Chatgpt</a>或<a href="https://claude.ai/" target="_blank" rel="noreferrer">Claude</a>一样，您也使用OpenAI和Anthropic托管的模型，因此这些提供商可能会收集您的提示和数据来培训和增强其模型的功能。如果您担心将数据发送到这些LLM提供者，您可以使用本地优先LLM工具以离线运行您的首选模型。当地第一LLM工具是一种工具，可让您在不使用网络的情况下聊天和测试模型。使用Lmstudio，Ollama和Jan等工具，您可以与您喜欢的任何型号进行聊天，例如DeepSeek R1型号100％离线。了解有关本地第一的更多信息LLM我们最近的<a href="https://getstream.io/blog/best-local-llm-tools/" target="_blank" rel="noreferrer">文章</a>和<a href="https://youtu.be/pyFRIlk4se0?si=Y2DjgHLH2YZ4OF1R" target="_blank" rel="noreferrer">YouTube教程</a>之一中的工具。</p><h1 id="必备当地人llmdeepseek-r1的工具" tabindex="-1">必备当地人LLMDeepSeek R1的工具 <a class="header-anchor" href="#必备当地人llmdeepseek-r1的工具" aria-label="Permalink to &quot;必备当地人LLMDeepSeek R1的工具&quot;">​</a></h1><p><img src="https://miro.medium.com/v2/resize:fit:1400/0*wEePc6qX6dPOTSHt" alt="img" loading="lazy"></p><p>与Lmstudio，Jan和Ollama在本地运行DeepSeek R1</p><p>自从DeepSeek R1模型发布以来，本地数量越来越多LLM下载和使用模型的平台而无需连接到Internet。以下是在撰写本文时可以使用R1脱机的三个最佳应用程序。我们将偶尔更新文章作为本地数量LLM工具支持R1增加。</p><h1 id="lmstudio" tabindex="-1">LMStudio <a class="header-anchor" href="#lmstudio" aria-label="Permalink to &quot;LMStudio&quot;">​</a></h1><p><img src="https://miro.medium.com/v2/resize:fit:1400/1*IvhmppMlz3f48knAdIqoAg.gif" alt="img" loading="lazy"></p><p>使用LMSTUDIO在本地运行DeepSeek R1</p><p>LMstudio提供了可以离线运行的DeepSeek R1的蒸馏版。首先，<a href="https://lmstudio.ai/" target="_blank" rel="noreferrer">下载</a>lmstudio，启动它，然后单击左面板上的**“发现”<strong>选项卡以下载，安装和运行任何蒸馏版R1。在<a href="https://youtube.com/shorts/51aYf_39sBU?si=Axkf1r1H_0S0SxFm" target="_blank" rel="noreferrer">YouTube</a>上</strong>使用LMSTUDIO在本地观看Run Run Run DeepSeek R1，**以便逐步快速指南。</p><h1 id="奥拉马" tabindex="-1">奥拉马 <a class="header-anchor" href="#奥拉马" aria-label="Permalink to &quot;奥拉马&quot;">​</a></h1><p><img src="https://miro.medium.com/v2/resize:fit:1400/0*2CoGo0NZVvT7u0kl" alt="img" loading="lazy"></p><p>下载Ollama</p><p>使用Ollama，您可以在没有网络的情况下使用单个命令运行DeepSeek R1型号100％。首先，安装<a href="https://ollama.com/" target="_blank" rel="noreferrer">Ollama</a> ，然后运行以下命令来拉动并运行DeepSeek R1模型：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>ollama run deepseek-r1</span></span></code></pre></div><p>您还可以拉动并运行DeepSeek R1型号的以下蒸馏Qwen和Llama版本。</p><p><strong>QWEN蒸馏DeepSeekr1型号</strong></p><ul><li><strong>DeepSeek-R1-Distill-Qwen-1.5b</strong> ： <code>ollama run deepseek-r1:1.5b</code></li><li><strong>DeepSeek-R1-Distill-Qwen-7b</strong> ： <code>ollama run deepseek-r1:7b</code></li><li><strong>DeepSeek-R1-Distill-Qwen-14b</strong> ： <code>ollama run deepseek-r1:14b</code></li><li><strong>DeepSeek-R1-Distill-Qwen-32b</strong> ： <code>ollama run deepseek-r1:32b</code></li></ul><p><strong>美洲驼蒸馏DeepSeekr1模型</strong></p><ul><li><strong>DeepSeek-R1-Distill-Lalama-8B</strong> ： <code>ollama run deepseek-r1:8b</code></li><li><strong>DeepSeek-R1-Distill-Lalama-70b</strong> ： <code>ollama run deepseek-r1:70b</code></li></ul><p>下面的预览展示了如何与Ollama一起运行<strong>DeepSeek-R1-Distill-Lalama-8b</strong> 。</p><p><img src="https://miro.medium.com/v2/resize:fit:1400/1*n6JPQJdCy-bdpgUXUb9Nuw.gif" alt="img" loading="lazy"></p><p>Ollama运行DeepSeek-R1</p><p>观看<strong>Run DeepSeek R1 + Ollama本地LLM</strong>在<a href="https://youtube.com/shorts/qd4Rm7kyksM?si=sDfvA3L52xpHC8TI" target="_blank" rel="noreferrer">YouTube</a>上的<strong>工具</strong>进行快速演练。</p><h1 id="jan-与deepseek-r1离线聊天" tabindex="-1">Jan：与DeepSeek R1离线聊天 <a class="header-anchor" href="#jan-与deepseek-r1离线聊天" aria-label="Permalink to &quot;Jan：与DeepSeek R1离线聊天&quot;">​</a></h1><p><img src="https://miro.medium.com/v2/resize:fit:1400/0*18XKjTNibGodIawx" alt="img" loading="lazy"></p><p>与Jan下载DeepSeek R1</p><p>Jan将自己描述为一种开源Chatgpt替代方案。这是当地的第一LLM运行DeepSeek R1型号100％离线的工具。使用JAN运行DeepSeek R1仅需要下图中所示的三个步骤。</p><p><img src="https://miro.medium.com/v2/resize:fit:1400/0*IXnHl-tuHbJ1bf44" alt="img" loading="lazy"></p><p>与Jan一起运行DeepSeek R1的步骤</p><p>下载<a href="https://jan.ai/" target="_blank" rel="noreferrer">Jan</a> ，然后前往左图上的<strong>Hub</strong>选项卡，以搜索并下载从<a href="https://huggingface.co/" target="_blank" rel="noreferrer">拥抱面</a>中的任何蒸馏R1 GGUF型号。</p><p><strong>DeepSeek R1 Qwen蒸馏型</strong></p><ul><li>1.5b： <a href="https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF" target="_blank" rel="noreferrer">https</a> ：<a href="//huggingface.co/bartowski/deepseek-r1-distill-qwen-1.5b-gguf" target="_blank" rel="noreferrer">//huggingface.co/bartowski/deepseek-r1-distill-qwen-1.5b-gguf</a></li><li>7b： <a href="https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF" target="_blank" rel="noreferrer">https</a> ：<a href="//hugingface.co/bartowski/deepseek-r1-distill-qwen-7b-gguf" target="_blank" rel="noreferrer">//hugingface.co/bartowski/deepseek-r1-distill-qwen-7b-gguf</a></li><li>14b： <a href="https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF" target="_blank" rel="noreferrer">https</a> ：<a href="//huggingface.co/bartowski/deepseek-r1-distill-qwen-14b-gguf" target="_blank" rel="noreferrer">//huggingface.co/bartowski/deepseek-r1-distill-qwen-14b-gguf</a></li><li>32B： <a href="https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-32B-GGUF" target="_blank" rel="noreferrer">https</a> ：<a href="//hugingface.co/bartowski/deepseek-r1-distill-qwen-32b-gguf" target="_blank" rel="noreferrer">//hugingface.co/bartowski/deepseek-r1-distill-qwen-32b-gguf</a></li></ul><p><strong>DeepSeek R1美洲驼蒸馏模型</strong></p><ul><li>8b： <a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF" target="_blank" rel="noreferrer">https</a> ：<a href="//huggingface.co/unsloth/deepseek-r1-distill-llama-8b-gguf" target="_blank" rel="noreferrer">//huggingface.co/unsloth/deepseek-r1-distill-llama-8b-gguf</a></li><li>70B： <a href="https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF" target="_blank" rel="noreferrer">https</a> ：<a href="//hugingface.co/unsloth/deepseek-r1-distill-lma-70b-gguf" target="_blank" rel="noreferrer">//hugingface.co/unsloth/deepseek-r1-distill-lma-70b-gguf</a></li></ul><p>一旦您下载了带有JAN的任何蒸馏R1型号，就可以按照以下预览进行运行。</p><p><img src="https://miro.medium.com/v2/resize:fit:1400/1*qauRmzn7xjPBcH0oejFLAg.gif" alt="img" loading="lazy"></p><p>与Jan一起运行DeepSeek R1</p><h1 id="其他替代方案-使用企业准备就绪llm托管r1托管" tabindex="-1">其他替代方案：使用企业准备就绪LLM托管R1托管 <a class="header-anchor" href="#其他替代方案-使用企业准备就绪llm托管r1托管" aria-label="Permalink to &quot;其他替代方案：使用企业准备就绪LLM托管R1托管&quot;">​</a></h1><p><img src="https://miro.medium.com/v2/resize:fit:1400/1*9r4fRCBgN-F2J6MEg4NyLg.png" alt="img" loading="lazy"></p><p>DeepSeek R1托管选项</p><p>尽管最近发布了DeepSeek R1模型，但有些值得信赖LLM托管平台支持它。如果您不想使用上面概述的离线方法，则可以从以下任何提供商访问模型。可能有几个LLM在此处陈述的托管平台中缺少平台。但是，以下是领先的平台，您可以在其中访问DeepSeek R1型号及其蒸馏器。</p><h1 id="格罗克" tabindex="-1">格罗克 <a class="header-anchor" href="#格罗克" aria-label="Permalink to &quot;格罗克&quot;">​</a></h1><p><img src="https://miro.medium.com/v2/resize:fit:1400/1*t7_tv6Ey2ElXQeiAt2XS7g.gif" alt="img" loading="lazy"></p><p>在Groq上运行DeepSeek R1</p><p>GROQ支持<code>DeepSeek-R1-Distill-Llama-70B</code>版本。要使用它，请访问<a href="https://groq.com/%E5%B9%B6%E7%9B%B4%E6%8E%A5%E5%9C%A8%E4%B8%BB%E9%A1%B5%E4%B8%8A%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%9E%8B%E3%80%82" target="_blank" rel="noreferrer">https://groq.com/并直接在主页上运行模型。</a> 另外，您可以通过单击主页右上角的<strong>DEV控制台</strong>按钮在Groq上运行R1模型，如下图所示。</p><p><img src="https://miro.medium.com/v2/resize:fit:1400/1*z-xnWPHT-dlvQc6B-6JA3Q.gif" alt="img" loading="lazy"></p><p>使用Groq Dev Console运行DeepSeek R1</p><h1 id="azure-ai铸造厂" tabindex="-1">Azure AI铸造厂 <a class="header-anchor" href="#azure-ai铸造厂" aria-label="Permalink to &quot;Azure AI铸造厂&quot;">​</a></h1><p><img src="https://miro.medium.com/v2/resize:fit:1400/1*Q-k_m5wqjLEj3ris38HIgg.gif" alt="img" loading="lazy"></p><p>访问Azure AI Foundry的DeepSeek R1</p><p>如上图所示，您可以在Microsoft的Aure AI Foundry上访问DeepSeek R1的蒸馏版。访问Azure AI Foundry网站以<a href="https://azure.microsoft.com/en-us/products/ai-foundry" target="_blank" rel="noreferrer">开始</a>。</p><p>其他受欢迎LLM托管平台您可以运行DeepSeek R1的蒸馏型号包括以下链接。</p><p><img src="https://miro.medium.com/v2/resize:fit:1400/0*Ah-fLpta18-57s-1" alt="img" loading="lazy"></p><p>deepseek r1一起。</p><ul><li><a href="https://fireworks.ai/" target="_blank" rel="noreferrer">https://fireworks.ai/</a></li><li><a href="https://www.together.ai/" target="_blank" rel="noreferrer">https://www.together.ai/</a></li><li><a href="https://openrouter.ai/" target="_blank" rel="noreferrer">https://openrouter.ai/</a></li><li><a href="https://chatllm.abacus.ai/" target="_blank" rel="noreferrer">https://chatllm.abacus.ai/</a></li><li><a href="https://chutes.ai/" target="_blank" rel="noreferrer">https://chutes.ai/</a></li></ul><h1 id="deepseek-r1和开源模型的未来" tabindex="-1">DeepSeek R1和开源模型的未来 <a class="header-anchor" href="#deepseek-r1和开源模型的未来" aria-label="Permalink to &quot;DeepSeek R1和开源模型的未来&quot;">​</a></h1><p>在本文中，您学会了如何使用<a href="https://getstream.io/blog/best-local-llm-tools/" target="_blank" rel="noreferrer">本地优先权</a>运行DeepSeek R1模型<a href="https://getstream.io/blog/best-local-llm-tools/" target="_blank" rel="noreferrer">LLM</a>Lmstudio，Ollama和Jan等<a href="https://getstream.io/blog/best-local-llm-tools/" target="_blank" rel="noreferrer">工具</a>。您还学会了如何使用可伸缩和企业准备LLM托管平台运行模型。 DeepSeek R1模型是OpenAI O1模型的绝佳替代方法，能够推理完成高度要求和合乎逻辑的任务。</p><p>在撰写本文时，可访问DeepSeek R1模型LLM托管平台，例如Azure <a href="https://azure.microsoft.com/en-us/products/ai-foundry" target="_blank" rel="noreferrer">AI Foundry</a>和<a href="https://groq.com/" target="_blank" rel="noreferrer">Groq</a> 。这些平台确保其托管语言模型的可靠性和安全性。将来，我们希望看到更多的公司和开源开发人员重现DeepSeek R1模型，并为不同的用例提供。另外，许多本地第一LLM工具和托管服务可以支持DeepSeek R1型号及其蒸馏版。</p><h2 id="联系我" tabindex="-1">联系我 <a class="header-anchor" href="#联系我" aria-label="Permalink to &quot;联系我&quot;">​</a></h2><p>最后，推荐大家关注一下开源项目：LangChat，Java生态下的AIGC大模型产品解决方案。</p><ul><li>LangChat产品官网：<a href="https://langchat.cn/" target="_blank" rel="noreferrer">https://langchat.cn/</a></li><li>Github: <a href="https://github.com/TyCoding/langchat" target="_blank" rel="noreferrer">https://github.com/TyCoding/langchat</a></li><li>Gitee: <a href="https://gitee.com/langchat/langchat" target="_blank" rel="noreferrer">https://gitee.com/langchat/langchat</a></li><li>微信：LangchainChat</li></ul><p><img src="http://cdn.tycoding.cn/docs/202502151026673.png" alt="iShot_2025-02-12_12.18.53" loading="lazy"></p>',86)]))}const d=r(o,[["render",i]]);export{c as __pageData,d as default};
