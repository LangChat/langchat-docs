import{_ as e,c as t,o,ag as r}from"./chunks/framework.ByciF0Oj.js";const n="/server/proxy1.png",g=JSON.parse('{"title":"在LangChat中使用第三方代理配置模型","description":"","frontmatter":{},"headers":[],"relativePath":"docs/start/models-proxy.md","filePath":"docs/start/models-proxy.md","lastUpdated":1739355454000}'),s={name:"docs/start/models-proxy.md"};function p(l,a,d,c,i,h){return o(),t("div",null,a[0]||(a[0]=[r('<h1 id="在langchat中使用第三方代理配置模型" tabindex="-1">在LangChat中使用第三方代理配置模型 <a class="header-anchor" href="#在langchat中使用第三方代理配置模型" aria-label="Permalink to &quot;在LangChat中使用第三方代理配置模型&quot;">​</a></h1><p>LangChat支持例如one api平台的代理接口方式，对于购买了第三方代理平台的同学非常有用。</p><p><strong>注意：</strong> 使用第三方平台代理，一定要在BaseUrl后面增加 <code>/v1</code> 后缀。</p><p><strong>注意：</strong> 使用第三方平台代理，一定要在BaseUrl后面增加 <code>/v1</code> 后缀。</p><p><strong>注意：</strong> 使用第三方平台代理，一定要在BaseUrl后面增加 <code>/v1</code> 后缀。</p><p>首先，第三方代理一般有两种接入方式：</p><ol><li>修改BaseUrl，统一使用OpenAI接口格式分发请求，无论什么模型都将使用统一的请求响应格式</li><li>仅修改BaseUrl，仍按照模型官方的接口格式转发请求</li></ol><h2 id="openai接口格式" tabindex="-1">OpenAI接口格式 <a class="header-anchor" href="#openai接口格式" aria-label="Permalink to &quot;OpenAI接口格式&quot;">​</a></h2><blockquote><p>注意：如上，这里说的是如果第三方代理使用OpenAI接口格式转发不同模型供应商接口格式的情况，在这种情况下，所有模型的请求响应接口都是安装OpenAI格式</p></blockquote><p>对于这种情况，只需要在LangChat模型配置页面中选择OpenAI配置不同的模型即可：（主要是遵循OpenAI接口格式，无论什么模型都在可以这里配置）</p><p><img src="'+n+'" alt="" loading="lazy"></p><h2 id="代理官方接口格式" tabindex="-1">代理官方接口格式 <a class="header-anchor" href="#代理官方接口格式" aria-label="Permalink to &quot;代理官方接口格式&quot;">​</a></h2><p>对于第二种情况，仅仅需要修改BaseUrl即可，然后不同的模型仍然在LangChat不同的模型供应商下面配置。</p>',13)]))}const m=e(s,[["render",p]]);export{g as __pageData,m as default};
