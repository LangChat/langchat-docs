<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>LangChat如何接入DeepSeek-R1模型 | LangChat Docs</title>
    <meta name="description" content="LangChat Project Document">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/assets/style.wFIoVwSX.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.Cjw_v9nb.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.BoyLCJnS.js">
    <link rel="modulepreload" href="/assets/chunks/framework.ByciF0Oj.js">
    <link rel="modulepreload" href="/assets/docs_exercise_langchat-deepseek-r1.md.BG1HS0Jq.lean.js">
    <link rel="shortcut icon" href="/favicon.png">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-ed470ed6><!--[--><!--]--><!--[--><span tabindex="-1" data-v-f444f753></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-f444f753>Skip to content</a><!--]--><!----><header class="VPNav" data-v-ed470ed6 data-v-f09ebc71><div class="VPNavBar" data-v-f09ebc71 data-v-1044928a><div class="wrapper" data-v-1044928a><div class="container" data-v-1044928a><div class="title" data-v-1044928a><div class="VPNavBarTitle has-sidebar" data-v-1044928a data-v-c494e956><a class="title" href="/" data-v-c494e956><!--[--><!--]--><!----><span data-v-c494e956>LangChat Docs</span><!--[--><!--]--></a></div></div><div class="content" data-v-1044928a><div class="content-body" data-v-1044928a><!--[--><!--]--><div class="VPNavBarSearch search" data-v-1044928a><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-1044928a data-v-51f1f89a><span id="main-nav-aria-label" class="visually-hidden" data-v-51f1f89a> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-51f1f89a data-v-2ebde2b3><!--[--><span data-v-2ebde2b3>LangChat</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink active" href="/docs/exercise/langchat-deepseek-r1.html" tabindex="0" data-v-51f1f89a data-v-2ebde2b3><!--[--><span data-v-2ebde2b3>LangChat文档</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-51f1f89a data-v-e2cb7df8><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-e2cb7df8><span class="text" data-v-e2cb7df8><!----><span data-v-e2cb7df8>在线预览</span><span class="vpi-chevron-down text-icon" data-v-e2cb7df8></span></span></button><div class="menu" data-v-e2cb7df8><div class="VPMenu" data-v-e2cb7df8 data-v-7750fdeb><div class="items" data-v-7750fdeb><!--[--><!--[--><div class="VPMenuLink" data-v-7750fdeb data-v-9a9070ef><a class="VPLink link vp-external-link-icon" href="https://langchat.cn/" target="_blank" rel="noreferrer" data-v-9a9070ef><!--[--><span data-v-9a9070ef>LangChat官网</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7750fdeb data-v-9a9070ef><a class="VPLink link vp-external-link-icon" href="http://backend.langchat.cn/" target="_blank" rel="noreferrer" data-v-9a9070ef><!--[--><span data-v-9a9070ef>LangChat后台预览</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7750fdeb data-v-9a9070ef><a class="VPLink link vp-external-link-icon" href="http://llm.langchat.cn" target="_blank" rel="noreferrer" data-v-9a9070ef><!--[--><span data-v-9a9070ef>LangChat LLM Ops</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7750fdeb data-v-9a9070ef><a class="VPLink link vp-external-link-icon" href="http://upms.langchat.cn" target="_blank" rel="noreferrer" data-v-9a9070ef><!--[--><span data-v-9a9070ef>LangChat UPMS Ops</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-1044928a data-v-adf2275d><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-adf2275d data-v-4cc58f0d data-v-f45163c9><span class="check" data-v-f45163c9><span class="icon" data-v-f45163c9><!--[--><span class="vpi-sun sun" data-v-4cc58f0d></span><span class="vpi-moon moon" data-v-4cc58f0d></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-1044928a data-v-17ef9bfb data-v-34ec2aad><!--[--><a class="VPSocialLink no-icon" href="https://github.com/TyCoding/langchat" aria-label="github" target="_blank" rel="noopener" data-v-34ec2aad data-v-6953ab1f><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-1044928a data-v-a84198a3 data-v-e2cb7df8><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-e2cb7df8><span class="vpi-more-horizontal icon" data-v-e2cb7df8></span></button><div class="menu" data-v-e2cb7df8><div class="VPMenu" data-v-e2cb7df8 data-v-7750fdeb><!----><!--[--><!--[--><!----><div class="group" data-v-a84198a3><div class="item appearance" data-v-a84198a3><p class="label" data-v-a84198a3>Appearance</p><div class="appearance-action" data-v-a84198a3><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-a84198a3 data-v-4cc58f0d data-v-f45163c9><span class="check" data-v-f45163c9><span class="icon" data-v-f45163c9><!--[--><span class="vpi-sun sun" data-v-4cc58f0d></span><span class="vpi-moon moon" data-v-4cc58f0d></span><!--]--></span></span></button></div></div></div><div class="group" data-v-a84198a3><div class="item social-links" data-v-a84198a3><div class="VPSocialLinks social-links-list" data-v-a84198a3 data-v-34ec2aad><!--[--><a class="VPSocialLink no-icon" href="https://github.com/TyCoding/langchat" aria-label="github" target="_blank" rel="noopener" data-v-34ec2aad data-v-6953ab1f><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-1044928a data-v-5598f36f><span class="container" data-v-5598f36f><span class="top" data-v-5598f36f></span><span class="middle" data-v-5598f36f></span><span class="bottom" data-v-5598f36f></span></span></button></div></div></div></div><div class="divider" data-v-1044928a><div class="divider-line" data-v-1044928a></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-ed470ed6 data-v-d1ebcfd2><div class="container" data-v-d1ebcfd2><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-d1ebcfd2><span class="vpi-align-left menu-icon" data-v-d1ebcfd2></span><span class="menu-text" data-v-d1ebcfd2>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-d1ebcfd2 data-v-8683af8e><button data-v-8683af8e>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-ed470ed6 data-v-b5cecb30><div class="curtain" data-v-b5cecb30></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-b5cecb30><span class="visually-hidden" id="sidebar-aria-label" data-v-b5cecb30> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-d47b2d47><section class="VPSidebarItem level-0 has-active" data-v-d47b2d47 data-v-f455531b><div class="item" role="button" tabindex="0" data-v-f455531b><div class="indicator" data-v-f455531b></div><h2 class="text" data-v-f455531b>LangChat实战</h2><!----></div><div class="items" data-v-f455531b><!--[--><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/exercise/langchat-deepseek-r1.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>DeepSeek-R1实战</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/exercise/oss-minio.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>使用Minio作为OSS</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/exercise/rag.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>LLM-RAG基础概念</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-d47b2d47><section class="VPSidebarItem level-0" data-v-d47b2d47 data-v-f455531b><div class="item" role="button" tabindex="0" data-v-f455531b><div class="indicator" data-v-f455531b></div><h2 class="text" data-v-f455531b>LangChat配置</h2><!----></div><div class="items" data-v-f455531b><!--[--><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/start/introduce.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>LangChat介绍</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/start/environment.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>环境准备</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/start/getting-started.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>快速开始</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/start/login.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>登录LangChat</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/start/models.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>模型配置</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/start/models-proxy.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>模型代理</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/start/knowledge.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>知识库</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/start/questions.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>常见问题</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-d47b2d47><section class="VPSidebarItem level-0" data-v-d47b2d47 data-v-f455531b><div class="item" role="button" tabindex="0" data-v-f455531b><div class="indicator" data-v-f455531b></div><h2 class="text" data-v-f455531b>LangChat部署</h2><!----></div><div class="items" data-v-f455531b><!--[--><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/deploy/deploy.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>LangChat部署教程</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-d47b2d47><section class="VPSidebarItem level-0" data-v-d47b2d47 data-v-f455531b><div class="item" role="button" tabindex="0" data-v-f455531b><div class="indicator" data-v-f455531b></div><h2 class="text" data-v-f455531b>推荐阅读</h2><!----></div><div class="items" data-v-f455531b><!--[--><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/chunking-strategies.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>大模型RAG中的分块策略</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/claude-3-7-sonnet.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>Claude 3.7 Sonnet强势来袭</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/deepseek-r1-architecture-and-training.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>DeepSeek R1架构和训练过程图解</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/deepseek-r1-distilled-models.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>DeepSeek-R1蒸馏模型</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/deepseek-r1-reasoning-capabilities-analysis.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>DeepSeek-R1的推理能力分析</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/deepseek-r1-tuning.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>DeepSeek-R1微调指南</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/distill-deepseek-r1-into-your-model.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>蒸馏DeepSeek-R1到自己的模型</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/guide-getting-started-with-cursor-and-deepseek-r1.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>Cursor + DeepSeek R1 使用指南</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/hardware-guide-for-llm-training-and-fine-tuning.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>大模型训练/微调硬件指南</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/markitdown-a-deep-dive.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>MarkItDown深入研究</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/reasoning-modes-vs-other-ai-models.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>推理模型 vs. 其他AI模型</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/the-ai-web-search-landscape.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>AI搜索引擎生态全景</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/top8-on-premise-plans-for-deepseek-r1.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>8个DeepSeek-R1私有化部署方案</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/top11-ai-chat-ui-for-developers.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>11个开发人员必备AI聊天界面</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/vllm-llm-local-inference-library.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>vLLM 大模型本地推理库</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/vllm-ollama-comprehensive-comparison.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>vLLM/ollama综合对比</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/vllm-vs-ollama.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>VLLM vs. Ollama</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-ed470ed6 data-v-ee4370c0><div class="VPDoc has-sidebar has-aside" data-v-ee4370c0 data-v-479a6568><!--[--><!--]--><div class="container" data-v-479a6568><div class="aside" data-v-479a6568><div class="aside-curtain" data-v-479a6568></div><div class="aside-container" data-v-479a6568><div class="aside-content" data-v-479a6568><div class="VPDocAside" data-v-479a6568 data-v-312359a6><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-312359a6 data-v-f9862b92><div class="content" data-v-f9862b92><div class="outline-marker" data-v-f9862b92></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-f9862b92>On this page</div><ul class="VPDocOutlineItem root" data-v-f9862b92 data-v-316b2cab><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-312359a6></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-479a6568><div class="content-container" data-v-479a6568><!--[--><!--]--><main class="main" data-v-479a6568><div style="position:relative;" class="vp-doc _docs_exercise_langchat-deepseek-r1" data-v-479a6568><div><h1 id="langchat如何接入deepseek-r1模型" tabindex="-1">LangChat如何接入DeepSeek-R1模型 <a class="header-anchor" href="#langchat如何接入deepseek-r1模型" aria-label="Permalink to &quot;LangChat如何接入DeepSeek-R1模型&quot;">​</a></h1><blockquote><p>本教程给使用LangChat的朋友学习如何本地部署DeepSeek-R1模型。以及如何使用LangChat的Agent功能构建知识库。</p></blockquote><h3 id="关于langchat" tabindex="-1">关于LangChat <a class="header-anchor" href="#关于langchat" aria-label="Permalink to &quot;关于LangChat&quot;">​</a></h3><p><strong>LangChat</strong> 是Java生态下企业级AIGC项目解决方案，集成RBAC和AIGC大模型能力，帮助企业快速定制AI知识库、企业AI机器人。</p><p><strong>支持的AI大模型：</strong> Gitee AI / 阿里通义 / 百度千帆 / DeepSeek / 抖音豆包 / 智谱清言 / 零一万物 / 讯飞星火 / OpenAI / Gemini / Ollama / Azure / Claude 等大模型。</p><ul><li>官网地址：<a href="http://langchat.cn/" target="_blank" rel="noreferrer">http://langchat.cn/</a></li></ul><p><strong>开源地址：</strong></p><ul><li>Gitee：<a href="https://gitee.com/langchat/langchat" target="_blank" rel="noreferrer">https://gitee.com/langchat/langchat</a></li><li>Github：<a href="https://github.com/tycoding/langchat" target="_blank" rel="noreferrer">https://github.com/tycoding/langchat</a></li></ul><h2 id="安装deepseek-r1" tabindex="-1">安装DeepSeek-R1 <a class="header-anchor" href="#安装deepseek-r1" aria-label="Permalink to &quot;安装DeepSeek-R1&quot;">​</a></h2><p>DeepSeek-R1是由DeepSeek公司推出的开源大模型，目前最强的低成本推理模型。</p><p><strong>注意：</strong> 这里仅介绍使用Ollama部署DeepSeek-R1模型，其他模型的部署方式可以参考官方文档。</p><h3 id="基础概念" tabindex="-1">基础概念 <a class="header-anchor" href="#基础概念" aria-label="Permalink to &quot;基础概念&quot;">​</a></h3><p>首先，必须给大家介绍一些基础的概念，避免大家有各种疑惑。</p><ol><li>Ollama是安装模型最简单的方式，不需要python、Docker以及其他复杂的过程。</li><li>无论本地安装DeepSeek-R1的哪个模型，其实区别都不大，因为都是阉割版的，所以大家不要被营销号带偏。</li><li>Ollama的<code>run</code>命令执行后，首先会检查本地有没有此模型，有就安装，没有就直接运行模型。（Ollama客户端兼容全平台，不是非要Linux服务器）</li><li>Ollama的<code>run</code>启动模型后会暴露 <strong>11434</strong> HTTP端口，其他所有的LLM OPS应用都是通过此端口和模型交互的，你可以访问 <a href="http://127.0.0.1:11434" target="_blank" rel="noreferrer">http://127.0.0.1:11434</a> 查看</li><li>DeepSeek-R1是推理模型，Embedding模型需要单独安装，这两者不是一个概念，如果需要知识库向量化，不应该用DeepSeek-R1</li><li>......</li></ol><h3 id="deepseek-r1下载哪个版本" tabindex="-1">DeepSeek-R1下载哪个版本？ <a class="header-anchor" href="#deepseek-r1下载哪个版本" aria-label="Permalink to &quot;DeepSeek-R1下载哪个版本？&quot;">​</a></h3><p><strong>实际上，本地测试而言，无论你下载哪个版本，最终的效果都是一样的</strong>。因为都是阉割版的小参数模型，所以我推荐各位安装 <strong>1.5B</strong> 或者 <strong>7/8B</strong> 测试即可。真实的场景下，还是推荐调用API。</p><p>Ollama地址：<a href="https://ollama.com/library/deepseek-r1" target="_blank" rel="noreferrer">https://ollama.com/library/deepseek-r1</a></p><p><img src="http://cdn.tycoding.cn/docs/202502111644203.png" alt="image-20250211164354673" loading="lazy"></p><p><strong>注意：</strong></p><p>运行 <code>ollama run deepseek-r1</code> 命令默认安装的 <strong>7B</strong> 版本。</p><ol><li>1.5B 模型，基本上任意笔记本都能安装</li><li>7/8B 模型，至少本地电脑有16G内存</li><li>14B以上模型，至少本地电脑有32G内存</li></ol><p><strong>作者本地电脑是 Macbook m3 16GB + 512版本</strong></p><h3 id="_1-安装ollama" tabindex="-1">1. 安装Ollama <a class="header-anchor" href="#_1-安装ollama" aria-label="Permalink to &quot;1. 安装Ollama&quot;">​</a></h3><p>Ollama官网地址 <a href="https://ollama.com/" target="_blank" rel="noreferrer">https://ollama.com/</a></p><p><img src="http://cdn.tycoding.cn/docs/202502111648168.png" alt="image-20250211164855062" loading="lazy"></p><p>这里会根据你的操作系统下载对应的安装包</p><p><strong>安装Ollama的步骤这里就不再解释了</strong></p><h3 id="_2-安装deepseek-r1" tabindex="-1">2. 安装DeepSeek-R1 <a class="header-anchor" href="#_2-安装deepseek-r1" aria-label="Permalink to &quot;2. 安装DeepSeek-R1&quot;">​</a></h3><blockquote><p>如果你本地电脑是 &gt;=16G内存，就用默认的安装命令，否则建议安装1.5B小模型</p></blockquote><p><img src="http://cdn.tycoding.cn/docs/202502111652139.png" alt="image-20250211165252956" loading="lazy"></p><p>因为作者是16G笔记本，所以这里直接按照默认的下载7B模型</p><p>正常情况下，如上图所示，本地电脑就已经安装好了DeepSeek-R1模型。</p><h3 id="_3-验证deepseek-r1模型是否启动" tabindex="-1">3. 验证DeepSeek-R1模型是否启动 <a class="header-anchor" href="#_3-验证deepseek-r1模型是否启动" aria-label="Permalink to &quot;3. 验证DeepSeek-R1模型是否启动&quot;">​</a></h3><p>如上，你可以在控制台直接交互。</p><p>另外本地访问 <a href="http://127.0.0.1:11434" target="_blank" rel="noreferrer">http://127.0.0.1:11434</a> 地址，能看到如下信息：</p><p><img src="http://cdn.tycoding.cn/docs/202502111702790.png" alt="image-20250211170217736" loading="lazy"></p><h2 id="启动langchat" tabindex="-1">启动LangChat <a class="header-anchor" href="#启动langchat" aria-label="Permalink to &quot;启动LangChat&quot;">​</a></h2><blockquote><p>注意：LangChat至少需要以下环境：</p><ol><li>MySQL8</li><li>JDK17+</li><li>PgVector等</li></ol></blockquote><p><strong>开源地址：</strong></p><ul><li>Gitee：<a href="https://gitee.com/langchat/langchat" target="_blank" rel="noreferrer">https://gitee.com/langchat/langchat</a></li><li>Github：<a href="https://github.com/tycoding/langchat" target="_blank" rel="noreferrer">https://github.com/tycoding/langchat</a></li><li>GitCode: <a href="https://gitcode.com/LangChat/LangChat" target="_blank" rel="noreferrer">https://gitcode.com/LangChat/LangChat</a></li></ul><p>首先本地IDEA打开LangChat项目（等待Maven加载完成）</p><p><img src="http://cdn.tycoding.cn/docs/202502111704697.png" alt="image-20250211170424610" loading="lazy"></p><h3 id="_1-执行数据库脚本" tabindex="-1">1. 执行数据库脚本 <a class="header-anchor" href="#_1-执行数据库脚本" aria-label="Permalink to &quot;1. 执行数据库脚本&quot;">​</a></h3><p><img src="http://cdn.tycoding.cn/docs/202502111705186.png" alt="image-20250211170540137" loading="lazy"></p><p>在docs/目录下找到<code>langchat.sql</code> 在MySQL中执行此脚本。</p><p><strong>注意：</strong> 此脚本包含了创建名为<code>langchat</code>的数据库（因此不需要手动创建数据库）</p><h3 id="_2-修改配置文件" tabindex="-1">2. 修改配置文件 <a class="header-anchor" href="#_2-修改配置文件" aria-label="Permalink to &quot;2. 修改配置文件&quot;">​</a></h3><p>首先你应该检查SpringBoot的<code>application-*.yml</code>配置文件</p><p><img src="http://cdn.tycoding.cn/docs/202502111707940.png" alt="image-20250211170752880" loading="lazy"></p><p><strong>必须修改：</strong></p><ol><li>MySQL连接信息</li><li>OSS信息（默认的<code>local</code>代表了使用tomcat的地址，当然建议使用阿里云或七牛云，或者本地用NGINX搭建本地文件服务器）</li></ol><h3 id="_3-安装pgvector" tabindex="-1">3. 安装PgVector <a class="header-anchor" href="#_3-安装pgvector" aria-label="Permalink to &quot;3. 安装PgVector&quot;">​</a></h3><blockquote><p>这里我只推荐Pgvector，不要用Redis，Pgvector可以用navicat等工具可视化查看数据表</p></blockquote><p>Pgvector官方仓库：<a href="https://github.com/pgvector/pgvector?tab=readme-ov-file#installation-notes---windows" target="_blank" rel="noreferrer">https://github.com/pgvector/pgvector?tab=readme-ov-file#installation-notes---windows</a></p><p>Postgres官网：<a href="https://postgresapp.com/downloads.html" target="_blank" rel="noreferrer">https://postgresapp.com/downloads.html</a></p><p>注意：安装Pgvector后仍需要有Postgres 15+基础环境。所以如果你是第一次安装，你需要安装两者才行。</p><p><img src="http://cdn.tycoding.cn/docs/202502111715986.png" alt="image-20250211171529888" loading="lazy"></p><p>因为我使用的Mac，所以有多种安装方式，如果不想麻烦可以用Docker</p><p><img src="http://cdn.tycoding.cn/docs/202502111716476.png" alt="image-20250211171648413" loading="lazy"></p><p>作者贴心的给大家编译了一个pgvector发布到了阿里云仓库，直接运行此compose也可启动，省去了上麦那一系列步骤</p><p>如果上面脚本执行成功，应该在数据库能看到<code>langchat</code></p><p><img src="http://cdn.tycoding.cn/docs/202502111718850.png" alt="image-20250211171821785" loading="lazy"></p><p><strong>注意：</strong></p><ol><li>如果是自己手动安装的Pgvector，请手动创建<code>langchat</code>数据库</li><li>尽量不要自己手动编译pgvector源码，太麻烦了</li></ol><h3 id="_4-运行langchat" tabindex="-1">4. 运行LangChat <a class="header-anchor" href="#_4-运行langchat" aria-label="Permalink to &quot;4. 运行LangChat&quot;">​</a></h3><p>上述配置完毕后，即可正常启动LangChat</p><p><img src="http://cdn.tycoding.cn/docs/202502111722185.png" alt="image-20250211172222107" loading="lazy"></p><p>启动成功后如上图，注意 <strong>当前环境是什么就代表用了哪个配置文件</strong></p><p><strong>运行langchat-ui</strong></p><p><img src="http://cdn.tycoding.cn/docs/202502111723098.png" alt="image-20250211172348017" loading="lazy"></p><h2 id="测试langchat" tabindex="-1">测试LangChat <a class="header-anchor" href="#测试langchat" aria-label="Permalink to &quot;测试LangChat&quot;">​</a></h2><p>首先进入到LangChat此页面</p><p><img src="http://cdn.tycoding.cn/docs/202502111727377.png" alt="image-20250211172719346" loading="lazy"></p><p>因为我们使用的Ollama部署的DeepSeek-R1模型，因此必须使用Ollama配置</p><ol><li>模型版本写： <code>deepseek-r1</code></li><li>BaseUlr写：<code>http://127.0.0.1:11434/</code></li><li>ApiKey任意填</li></ol><p><img src="http://cdn.tycoding.cn/docs/202502111729516.png" alt="image-20250211172945369" loading="lazy"></p><h3 id="测试langchat聊天功能" tabindex="-1">测试LangChat聊天功能 <a class="header-anchor" href="#测试langchat聊天功能" aria-label="Permalink to &quot;测试LangChat聊天功能&quot;">​</a></h3><blockquote><p>到此为止，DeepSeek-R1模型已经启动并配置好，我们先测试Chat基础功能</p></blockquote><p><img src="http://cdn.tycoding.cn/docs/202502111731492.png" alt="image-20250211173131416" loading="lazy"></p><p><img src="http://cdn.tycoding.cn/docs/202502111733161.png" alt="image-20250211173310009" loading="lazy"></p><p>如上，接口已经掉通了。</p><p><strong>注意： <code>&lt;think&gt;</code>是DeekSeek-R1的推理过程，因为他是非标准的数据格式，后面LangChat会做前端适配</strong></p><h2 id="配置langchat知识库" tabindex="-1">配置LangChat知识库 <a class="header-anchor" href="#配置langchat知识库" aria-label="Permalink to &quot;配置LangChat知识库&quot;">​</a></h2><blockquote><p>首先你需要安装好Pgvector和OSS</p></blockquote><h3 id="_1-配置向量数据库" tabindex="-1">1. 配置向量数据库 <a class="header-anchor" href="#_1-配置向量数据库" aria-label="Permalink to &quot;1. 配置向量数据库&quot;">​</a></h3><p><img src="http://cdn.tycoding.cn/docs/202502111736653.png" alt="image-20250211173652563" loading="lazy"></p><p><strong>注意！注意！注意！</strong></p><p><strong>建议不要修改向量维度这个参数，向量数据表一旦初始化，此表的向量维度就固定了，只能接受指定向量的数据，因此仅在LangChat前端修改是无效的（需要删除原表）</strong></p><h3 id="_2-本地下载embedding模型" tabindex="-1">2. 本地下载Embedding模型 <a class="header-anchor" href="#_2-本地下载embedding模型" aria-label="Permalink to &quot;2. 本地下载Embedding模型&quot;">​</a></h3><blockquote><p>当然我建议大家直接使用阿里云、百度、智谱的Embedding模型，这样只需要配置ApiKey即可，但是很多朋友可能还想想本地部署，这里教大家。</p></blockquote><p>同样，使用Ollama下载Embedding模型</p><p>进入官网：<a href="https://ollama.com/search?c=embedding" target="_blank" rel="noreferrer">https://ollama.com/search?c=embedding</a> 我们找到排名第一的Embedding模型</p><p><img src="http://cdn.tycoding.cn/docs/202502112049972.png" alt="image-20250211204936831" loading="lazy"></p><p>执行命令 <code>ollama pull nomic-embed-text</code></p><p><img src="http://cdn.tycoding.cn/docs/202502112052648.png" alt="image-20250211205200587" loading="lazy"></p><p><img src="http://cdn.tycoding.cn/docs/202502112052670.png" alt="image-20250211205253605" loading="lazy"></p><p>如上结果，Ollama模型很小，很快运行结束，但是不要尝试执行<code>ollama run xxx</code>，因为<code>run</code>命令是针对Chat模型的，这里是Embedding模型</p><p><strong>Ollama Pull 了Embedding模型就自动启用了，不需要任何其他命令加载</strong></p><p>我们可以通过<code>ollama list</code>查看到下载的模型</p><p><img src="http://cdn.tycoding.cn/docs/202502112056210.png" alt="image-20250211205634137" loading="lazy"></p><h3 id="_3-测试embedding模型" tabindex="-1">3. 测试Embedding模型 <a class="header-anchor" href="#_3-测试embedding模型" aria-label="Permalink to &quot;3. 测试Embedding模型&quot;">​</a></h3><p>执行如下脚本</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> http://localhost:11434/api/embeddings</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -d</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;model&quot;: &quot;nomic-embed-text&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;prompt&quot;: &quot;The sky is blue because of Rayleigh scattering&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}&#39;</span></span></code></pre></div><p><img src="http://cdn.tycoding.cn/docs/202502112058680.png" alt="image-20250211205831626" loading="lazy"></p><p>如上说明Embedding模型正在运行，并且模型名称是<code>nomic-embed-text</code>，访问地址是：<code>http://localhost:11434/api/embeddings</code></p><h3 id="_4-langchat配置embedding" tabindex="-1">4. LangChat配置Embedding <a class="header-anchor" href="#_4-langchat配置embedding" aria-label="Permalink to &quot;4. LangChat配置Embedding&quot;">​</a></h3><p><img src="http://cdn.tycoding.cn/docs/202502112105200.png" alt="image-20250211210510117" loading="lazy"></p><ul><li>模型版本填 <code>nomic-embed-text</code> （Select中输入并回车即可）</li><li>BaseUrl填写：<code>http://localhost:11434/</code></li></ul><h3 id="_5-创建langchat知识库" tabindex="-1">5. 创建LangChat知识库 <a class="header-anchor" href="#_5-创建langchat知识库" aria-label="Permalink to &quot;5. 创建LangChat知识库&quot;">​</a></h3><p>知识库中只关联Embedding数据库和Embedding模型</p><p><img src="http://cdn.tycoding.cn/docs/202502112107089.png" alt="image-20250211210743993" loading="lazy"></p><h3 id="_6-导入知识库文档" tabindex="-1">6. 导入知识库文档 <a class="header-anchor" href="#_6-导入知识库文档" aria-label="Permalink to &quot;6. 导入知识库文档&quot;">​</a></h3><p>在上面配置好<code>nomic-embed-text</code>模型后，往知识库导入文档</p><p><img src="http://cdn.tycoding.cn/docs/202502112202699.png" alt="image-20250211220225536" loading="lazy"></p><p>如果你是按照上面步骤的Embedding模型，你应该会收到如下向量化失败错误：</p><p><img src="http://cdn.tycoding.cn/docs/202502112203163.png" alt="image-20250211220330043" loading="lazy"></p><h3 id="为什么报错error-expected-1024" tabindex="-1">为什么报错ERROR: expected 1024 <a class="header-anchor" href="#为什么报错error-expected-1024" aria-label="Permalink to &quot;为什么报错ERROR: expected 1024&quot;">​</a></h3><p>因为我们在LangChat配置的 <strong>1024向量纬度</strong> 的数据库，所以生成的表也只接收1024维度的数据。</p><p><strong>但是，</strong> <code>nomic-embed-text</code> 模型只能生成768维度的数据，并不是生成1024维度的数据（当然如果你使用公有云模型，他们的模型一般都能支持生成多维度的数据768、1024、1536等等）</p><p><strong>只不过我们下载的模型只支持生成单维度的向量数据。</strong></p><p><img src="http://cdn.tycoding.cn/docs/202502112211381.png" alt="image-20250211221101301" loading="lazy"></p><p>从上表中你可以查看到不同模型能生成什么维度的数据。</p><p>例如阿里云的文档中，关于Embedding模型的定义如下：（你在最初阶段就应该考虑哪种模型兼容哪种向量维度）</p><p><img src="http://cdn.tycoding.cn/docs/202502112251834.png" alt="image-20250211225113717" loading="lazy"></p><p><strong>遇到这种情况怎么处理？</strong></p><p>无论用的哪个模型，如果向量维度一旦不匹配，就必然会出现类似此报错信息。按照如下步骤开始解决此问题：（我们的前提是仍用本地的Embedding模型，当然你换一个能输出1024维度的Embedding模型也是可以的）</p><ol><li>使用如Navicat等客户端工具删除<code>langchat</code>库中的表</li></ol><p><img src="http://cdn.tycoding.cn/docs/202502112254845.png" alt="image-20250211225400733" loading="lazy"></p><ol start="2"><li>在LangChat管理端修改 <strong>向量数据库的纬度配置</strong> ，如下修改为768维度</li></ol><p><img src="http://cdn.tycoding.cn/docs/202502112303718.png" alt="image-20250211230358598" loading="lazy"></p><ol start="3"><li>重启LangChat后端项目，系统会重新生成此表，并接收768维度的向量</li></ol><p>正常情况，重启后，我们就可以重新导入文档进行向量化了。</p><h3 id="_7-重新导入知识库文档" tabindex="-1">7. 重新导入知识库文档 <a class="header-anchor" href="#_7-重新导入知识库文档" aria-label="Permalink to &quot;7. 重新导入知识库文档&quot;">​</a></h3><p>我们准备如下这个txt文档</p><p><img src="http://cdn.tycoding.cn/docs/202502112316282.png" alt="image-20250211231626120" loading="lazy"></p><p>正常情况，后端会提示向量化成功，会有如下日志：</p><p><img src="http://cdn.tycoding.cn/docs/202502112320265.png" alt="image-20250211232040168" loading="lazy"></p><p>在向量数据库中，能看到如下分段信息：</p><p><img src="http://cdn.tycoding.cn/docs/202502112319952.png" alt="image-20250211231945871" loading="lazy"></p><h3 id="_8-向量搜索测试" tabindex="-1">8. 向量搜索测试 <a class="header-anchor" href="#_8-向量搜索测试" aria-label="Permalink to &quot;8. 向量搜索测试&quot;">​</a></h3><p>正常情况，如果向量化成功，你能在LangChat页面看到如下切面信息：</p><p><img src="http://cdn.tycoding.cn/docs/202502112322040.png" alt="image-20250211232251955" loading="lazy"></p><p>那我们进行一下向量检索测试：</p><p><img src="http://cdn.tycoding.cn/docs/202502112323154.png" alt="image-20250211232340080" loading="lazy"></p><blockquote><p>到此为止，LangChat知识库配置已经结束</p></blockquote><h2 id="创建langchat-ai应用" tabindex="-1">创建LangChat AI应用 <a class="header-anchor" href="#创建langchat-ai应用" aria-label="Permalink to &quot;创建LangChat AI应用&quot;">​</a></h2><p>上面知识库配置成功后，下面开始创建LangChat AI应用</p><p><img src="http://cdn.tycoding.cn/docs/202502112325631.png" alt="image-20250211232536523" loading="lazy"></p><p>如下所示创建AI应用，这里进需要关联我们刚才设置的DeepSeek-R1模型</p><h3 id="配置langchat应用" tabindex="-1">配置LangChat应用 <a class="header-anchor" href="#配置langchat应用" aria-label="Permalink to &quot;配置LangChat应用&quot;">​</a></h3><p>创建LangChat应用后，关联刚才创建的知识库，即可进行知识库问答了</p><p><img src="http://cdn.tycoding.cn/docs/202502121128775.png" alt="image-20250212112844178" loading="lazy"></p><p>关联好我们创建的知识库后，直接测试就能引用知识库的内容了</p><h3 id="测试langchat应用" tabindex="-1">测试LangChat应用 <a class="header-anchor" href="#测试langchat应用" aria-label="Permalink to &quot;测试LangChat应用&quot;">​</a></h3><p><img src="http://cdn.tycoding.cn/docs/202502121131192.png" alt="image-20250212113128034" loading="lazy"></p><p>如上，说明了他刚才引用了我们上传的<code>langchat.txt</code>文档</p><h3 id="验证是否查询向量文本" tabindex="-1">验证是否查询向量文本？ <a class="header-anchor" href="#验证是否查询向量文本" aria-label="Permalink to &quot;验证是否查询向量文本？&quot;">​</a></h3><p><strong>验证此回答是否查询了知识库的向量信息？</strong></p><ol><li>我们可以在控制台看到如下打印日志：</li></ol><p><img src="http://cdn.tycoding.cn/docs/202502121134946.png" alt="image-20250212113459881" loading="lazy"></p><p>后面的部分就是引用的知识库文档</p><ol start="2"><li>可以拿未配置知识库的普通聊天做测试</li></ol><p><img src="http://cdn.tycoding.cn/docs/202502121137702.png" alt="image-20250212113735608" loading="lazy"></p><p>可以看到未配置知识库，是不知道LangChat是什么的。</p></div></div></main><footer class="VPDocFooter" data-v-479a6568 data-v-a9f44413><!--[--><!--]--><div class="edit-info" data-v-a9f44413><!----><div class="last-updated" data-v-a9f44413><p class="VPLastUpdated" data-v-a9f44413 data-v-0500d987>Last updated: <time datetime="2025-02-12T10:17:34.000Z" data-v-0500d987></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-a9f44413><span class="visually-hidden" id="doc-footer-aria-label" data-v-a9f44413>Pager</span><div class="pager" data-v-a9f44413><!----></div><div class="pager" data-v-a9f44413><a class="VPLink link pager-link next" href="/docs/exercise/oss-minio.html" data-v-a9f44413><!--[--><span class="desc" data-v-a9f44413>Next page</span><span class="title" data-v-a9f44413>使用Minio作为OSS</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"docs_deploy_deploy.md\":\"Dg2bS27a\",\"docs_exercise_langchat-deepseek-r1.md\":\"BG1HS0Jq\",\"docs_exercise_oss-minio.md\":\"BKGFqqWT\",\"docs_exercise_rag.md\":\"YedSmvND\",\"docs_other_bank.md\":\"5tiAkFLT\",\"docs_other_chunking-strategies.md\":\"gV_LewAr\",\"docs_other_claude-3-7-sonnet.md\":\"Dj1Hhibl\",\"docs_other_deepseek-r1-architecture-and-training.md\":\"0Z6H9Kbq\",\"docs_other_deepseek-r1-distilled-models.md\":\"4nK05JS_\",\"docs_other_deepseek-r1-reasoning-capabilities-analysis.md\":\"B5CK-D6e\",\"docs_other_deepseek-r1-tuning.md\":\"CdRsqumj\",\"docs_other_distill-deepseek-r1-into-your-model.md\":\"DdEwVv1R\",\"docs_other_guide-getting-started-with-cursor-and-deepseek-r1.md\":\"CrF2GZkG\",\"docs_other_hardware-guide-for-llm-training-and-fine-tuning.md\":\"CyLRPjt8\",\"docs_other_markitdown-a-deep-dive.md\":\"Cs6UPOgy\",\"docs_other_reasoning-modes-vs-other-ai-models.md\":\"BmYH1TUj\",\"docs_other_the-ai-web-search-landscape.md\":\"C4xGfer5\",\"docs_other_top11-ai-chat-ui-for-developers.md\":\"CSnixW_I\",\"docs_other_top8-on-premise-plans-for-deepseek-r1.md\":\"BroA2pse\",\"docs_other_vllm-llm-local-inference-library.md\":\"7uM8sIzs\",\"docs_other_vllm-ollama-comprehensive-comparison.md\":\"D9IVeAkT\",\"docs_other_vllm-vs-ollama.md\":\"BRFYvY27\",\"docs_start_environment.md\":\"DkSgePUm\",\"docs_start_getting-started.md\":\"B8WJQYY8\",\"docs_start_introduce.md\":\"CgbmexuX\",\"docs_start_knowledge.md\":\"Ce41O1tK\",\"docs_start_login.md\":\"DWOURVt-\",\"docs_start_models-proxy.md\":\"6b6woVJN\",\"docs_start_models.md\":\"Ds4X4F2M\",\"docs_start_questions.md\":\"AMWgx7AD\",\"index.md\":\"n9TEBaM7\",\"readme.md\":\"DKy5OORU\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"LangChat Docs\",\"description\":\"LangChat Project Document\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"outline\":{\"level\":\"deep\"},\"search\":{\"provider\":\"local\"},\"nav\":[{\"text\":\"LangChat\",\"link\":\"/\"},{\"text\":\"LangChat文档\",\"link\":\"/docs/exercise/langchat-deepseek-r1\",\"activeMatch\":\"/docs\"},{\"text\":\"在线预览\",\"items\":[{\"text\":\"LangChat官网\",\"link\":\"https://langchat.cn/\"},{\"text\":\"LangChat后台预览\",\"link\":\"http://backend.langchat.cn/\"},{\"text\":\"LangChat LLM Ops\",\"link\":\"http://llm.langchat.cn\"},{\"text\":\"LangChat UPMS Ops\",\"link\":\"http://upms.langchat.cn\"}]}],\"sidebar\":{\"/docs\":[{\"text\":\"LangChat实战\",\"items\":[{\"text\":\"DeepSeek-R1实战\",\"link\":\"/docs/exercise/langchat-deepseek-r1\"},{\"text\":\"使用Minio作为OSS\",\"link\":\"/docs/exercise/oss-minio\"},{\"text\":\"LLM-RAG基础概念\",\"link\":\"/docs/exercise/rag\"}]},{\"text\":\"LangChat配置\",\"items\":[{\"text\":\"LangChat介绍\",\"link\":\"/docs/start/introduce\"},{\"text\":\"环境准备\",\"link\":\"/docs/start/environment\"},{\"text\":\"快速开始\",\"link\":\"/docs/start/getting-started\"},{\"text\":\"登录LangChat\",\"link\":\"/docs/start/login\"},{\"text\":\"模型配置\",\"link\":\"/docs/start/models\"},{\"text\":\"模型代理\",\"link\":\"/docs/start/models-proxy\"},{\"text\":\"知识库\",\"link\":\"/docs/start/knowledge\"},{\"text\":\"常见问题\",\"link\":\"/docs/start/questions\"}]},{\"text\":\"LangChat部署\",\"items\":[{\"text\":\"LangChat部署教程\",\"link\":\"/docs/deploy/deploy\"}]},{\"text\":\"推荐阅读\",\"items\":[{\"text\":\"大模型RAG中的分块策略\",\"link\":\"/docs/other/chunking-strategies\"},{\"text\":\"Claude 3.7 Sonnet强势来袭\",\"link\":\"/docs/other/claude-3-7-sonnet\"},{\"text\":\"DeepSeek R1架构和训练过程图解\",\"link\":\"/docs/other/deepseek-r1-architecture-and-training\"},{\"text\":\"DeepSeek-R1蒸馏模型\",\"link\":\"/docs/other/deepseek-r1-distilled-models\"},{\"text\":\"DeepSeek-R1的推理能力分析\",\"link\":\"/docs/other/deepseek-r1-reasoning-capabilities-analysis\"},{\"text\":\"DeepSeek-R1微调指南\",\"link\":\"/docs/other/deepseek-r1-tuning\"},{\"text\":\"蒸馏DeepSeek-R1到自己的模型\",\"link\":\"/docs/other/distill-deepseek-r1-into-your-model\"},{\"text\":\"Cursor + DeepSeek R1 使用指南\",\"link\":\"/docs/other/guide-getting-started-with-cursor-and-deepseek-r1\"},{\"text\":\"大模型训练/微调硬件指南\",\"link\":\"/docs/other/hardware-guide-for-llm-training-and-fine-tuning\"},{\"text\":\"MarkItDown深入研究\",\"link\":\"/docs/other/markitdown-a-deep-dive\"},{\"text\":\"推理模型 vs. 其他AI模型\",\"link\":\"/docs/other/reasoning-modes-vs-other-ai-models\"},{\"text\":\"AI搜索引擎生态全景\",\"link\":\"/docs/other/the-ai-web-search-landscape\"},{\"text\":\"8个DeepSeek-R1私有化部署方案\",\"link\":\"/docs/other/top8-on-premise-plans-for-deepseek-r1\"},{\"text\":\"11个开发人员必备AI聊天界面\",\"link\":\"/docs/other/top11-ai-chat-ui-for-developers\"},{\"text\":\"vLLM 大模型本地推理库\",\"link\":\"/docs/other/vllm-llm-local-inference-library\"},{\"text\":\"vLLM/ollama综合对比\",\"link\":\"/docs/other/vllm-ollama-comprehensive-comparison\"},{\"text\":\"VLLM vs. Ollama\",\"link\":\"/docs/other/vllm-vs-ollama\"}]}]},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/TyCoding/langchat\"}]},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>