<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>什么是分块？ | LangChat Docs</title>
    <meta name="description" content="LangChat Project Document">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/assets/style.wFIoVwSX.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.Cjw_v9nb.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.BoyLCJnS.js">
    <link rel="modulepreload" href="/assets/chunks/framework.ByciF0Oj.js">
    <link rel="modulepreload" href="/assets/docs_other_chunking-strategies.md.gV_LewAr.lean.js">
    <link rel="shortcut icon" href="/favicon.png">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-ed470ed6><!--[--><!--]--><!--[--><span tabindex="-1" data-v-f444f753></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-f444f753>Skip to content</a><!--]--><!----><header class="VPNav" data-v-ed470ed6 data-v-f09ebc71><div class="VPNavBar" data-v-f09ebc71 data-v-1044928a><div class="wrapper" data-v-1044928a><div class="container" data-v-1044928a><div class="title" data-v-1044928a><div class="VPNavBarTitle has-sidebar" data-v-1044928a data-v-c494e956><a class="title" href="/" data-v-c494e956><!--[--><!--]--><!----><span data-v-c494e956>LangChat Docs</span><!--[--><!--]--></a></div></div><div class="content" data-v-1044928a><div class="content-body" data-v-1044928a><!--[--><!--]--><div class="VPNavBarSearch search" data-v-1044928a><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-1044928a data-v-51f1f89a><span id="main-nav-aria-label" class="visually-hidden" data-v-51f1f89a> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-51f1f89a data-v-2ebde2b3><!--[--><span data-v-2ebde2b3>LangChat</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink active" href="/docs/exercise/langchat-deepseek-r1.html" tabindex="0" data-v-51f1f89a data-v-2ebde2b3><!--[--><span data-v-2ebde2b3>LangChat文档</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-51f1f89a data-v-e2cb7df8><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-e2cb7df8><span class="text" data-v-e2cb7df8><!----><span data-v-e2cb7df8>在线预览</span><span class="vpi-chevron-down text-icon" data-v-e2cb7df8></span></span></button><div class="menu" data-v-e2cb7df8><div class="VPMenu" data-v-e2cb7df8 data-v-7750fdeb><div class="items" data-v-7750fdeb><!--[--><!--[--><div class="VPMenuLink" data-v-7750fdeb data-v-9a9070ef><a class="VPLink link vp-external-link-icon" href="https://langchat.cn/" target="_blank" rel="noreferrer" data-v-9a9070ef><!--[--><span data-v-9a9070ef>LangChat官网</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7750fdeb data-v-9a9070ef><a class="VPLink link vp-external-link-icon" href="http://backend.langchat.cn/" target="_blank" rel="noreferrer" data-v-9a9070ef><!--[--><span data-v-9a9070ef>LangChat后台预览</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7750fdeb data-v-9a9070ef><a class="VPLink link vp-external-link-icon" href="http://llm.langchat.cn" target="_blank" rel="noreferrer" data-v-9a9070ef><!--[--><span data-v-9a9070ef>LangChat LLM Ops</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7750fdeb data-v-9a9070ef><a class="VPLink link vp-external-link-icon" href="http://upms.langchat.cn" target="_blank" rel="noreferrer" data-v-9a9070ef><!--[--><span data-v-9a9070ef>LangChat UPMS Ops</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-1044928a data-v-adf2275d><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-adf2275d data-v-4cc58f0d data-v-f45163c9><span class="check" data-v-f45163c9><span class="icon" data-v-f45163c9><!--[--><span class="vpi-sun sun" data-v-4cc58f0d></span><span class="vpi-moon moon" data-v-4cc58f0d></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-1044928a data-v-17ef9bfb data-v-34ec2aad><!--[--><a class="VPSocialLink no-icon" href="https://github.com/TyCoding/langchat" aria-label="github" target="_blank" rel="noopener" data-v-34ec2aad data-v-6953ab1f><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-1044928a data-v-a84198a3 data-v-e2cb7df8><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-e2cb7df8><span class="vpi-more-horizontal icon" data-v-e2cb7df8></span></button><div class="menu" data-v-e2cb7df8><div class="VPMenu" data-v-e2cb7df8 data-v-7750fdeb><!----><!--[--><!--[--><!----><div class="group" data-v-a84198a3><div class="item appearance" data-v-a84198a3><p class="label" data-v-a84198a3>Appearance</p><div class="appearance-action" data-v-a84198a3><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-a84198a3 data-v-4cc58f0d data-v-f45163c9><span class="check" data-v-f45163c9><span class="icon" data-v-f45163c9><!--[--><span class="vpi-sun sun" data-v-4cc58f0d></span><span class="vpi-moon moon" data-v-4cc58f0d></span><!--]--></span></span></button></div></div></div><div class="group" data-v-a84198a3><div class="item social-links" data-v-a84198a3><div class="VPSocialLinks social-links-list" data-v-a84198a3 data-v-34ec2aad><!--[--><a class="VPSocialLink no-icon" href="https://github.com/TyCoding/langchat" aria-label="github" target="_blank" rel="noopener" data-v-34ec2aad data-v-6953ab1f><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-1044928a data-v-5598f36f><span class="container" data-v-5598f36f><span class="top" data-v-5598f36f></span><span class="middle" data-v-5598f36f></span><span class="bottom" data-v-5598f36f></span></span></button></div></div></div></div><div class="divider" data-v-1044928a><div class="divider-line" data-v-1044928a></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-ed470ed6 data-v-d1ebcfd2><div class="container" data-v-d1ebcfd2><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-d1ebcfd2><span class="vpi-align-left menu-icon" data-v-d1ebcfd2></span><span class="menu-text" data-v-d1ebcfd2>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-d1ebcfd2 data-v-8683af8e><button data-v-8683af8e>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-ed470ed6 data-v-b5cecb30><div class="curtain" data-v-b5cecb30></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-b5cecb30><span class="visually-hidden" id="sidebar-aria-label" data-v-b5cecb30> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-d47b2d47><section class="VPSidebarItem level-0" data-v-d47b2d47 data-v-f455531b><div class="item" role="button" tabindex="0" data-v-f455531b><div class="indicator" data-v-f455531b></div><h2 class="text" data-v-f455531b>LangChat实战</h2><!----></div><div class="items" data-v-f455531b><!--[--><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/exercise/langchat-deepseek-r1.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>DeepSeek-R1实战</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/exercise/oss-minio.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>使用Minio作为OSS</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/exercise/rag.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>LLM-RAG基础概念</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-d47b2d47><section class="VPSidebarItem level-0" data-v-d47b2d47 data-v-f455531b><div class="item" role="button" tabindex="0" data-v-f455531b><div class="indicator" data-v-f455531b></div><h2 class="text" data-v-f455531b>LangChat配置</h2><!----></div><div class="items" data-v-f455531b><!--[--><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/start/introduce.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>LangChat介绍</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/start/environment.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>环境准备</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/start/getting-started.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>快速开始</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/start/login.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>登录LangChat</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/start/models.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>模型配置</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/start/models-proxy.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>模型代理</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/start/knowledge.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>知识库</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/start/questions.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>常见问题</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-d47b2d47><section class="VPSidebarItem level-0" data-v-d47b2d47 data-v-f455531b><div class="item" role="button" tabindex="0" data-v-f455531b><div class="indicator" data-v-f455531b></div><h2 class="text" data-v-f455531b>LangChat部署</h2><!----></div><div class="items" data-v-f455531b><!--[--><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/deploy/deploy.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>LangChat部署教程</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-d47b2d47><section class="VPSidebarItem level-0 has-active" data-v-d47b2d47 data-v-f455531b><div class="item" role="button" tabindex="0" data-v-f455531b><div class="indicator" data-v-f455531b></div><h2 class="text" data-v-f455531b>推荐阅读</h2><!----></div><div class="items" data-v-f455531b><!--[--><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/chunking-strategies.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>大模型RAG中的分块策略</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/claude-3-7-sonnet.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>Claude 3.7 Sonnet强势来袭</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/deepseek-r1-architecture-and-training.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>DeepSeek R1架构和训练过程图解</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/deepseek-r1-distilled-models.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>DeepSeek-R1蒸馏模型</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/deepseek-r1-reasoning-capabilities-analysis.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>DeepSeek-R1的推理能力分析</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/deepseek-r1-tuning.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>DeepSeek-R1微调指南</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/distill-deepseek-r1-into-your-model.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>蒸馏DeepSeek-R1到自己的模型</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/guide-getting-started-with-cursor-and-deepseek-r1.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>Cursor + DeepSeek R1 使用指南</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/hardware-guide-for-llm-training-and-fine-tuning.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>大模型训练/微调硬件指南</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/markitdown-a-deep-dive.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>MarkItDown深入研究</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/reasoning-modes-vs-other-ai-models.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>推理模型 vs. 其他AI模型</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/the-ai-web-search-landscape.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>AI搜索引擎生态全景</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/top8-on-premise-plans-for-deepseek-r1.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>8个DeepSeek-R1私有化部署方案</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/top11-ai-chat-ui-for-developers.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>11个开发人员必备AI聊天界面</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/vllm-llm-local-inference-library.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>vLLM 大模型本地推理库</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/vllm-ollama-comprehensive-comparison.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>vLLM/ollama综合对比</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f455531b data-v-f455531b><div class="item" data-v-f455531b><div class="indicator" data-v-f455531b></div><a class="VPLink link link" href="/docs/other/vllm-vs-ollama.html" data-v-f455531b><!--[--><p class="text" data-v-f455531b>VLLM vs. Ollama</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-ed470ed6 data-v-ee4370c0><div class="VPDoc has-sidebar has-aside" data-v-ee4370c0 data-v-479a6568><!--[--><!--]--><div class="container" data-v-479a6568><div class="aside" data-v-479a6568><div class="aside-curtain" data-v-479a6568></div><div class="aside-container" data-v-479a6568><div class="aside-content" data-v-479a6568><div class="VPDocAside" data-v-479a6568 data-v-312359a6><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-312359a6 data-v-f9862b92><div class="content" data-v-f9862b92><div class="outline-marker" data-v-f9862b92></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-f9862b92>On this page</div><ul class="VPDocOutlineItem root" data-v-f9862b92 data-v-316b2cab><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-312359a6></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-479a6568><div class="content-container" data-v-479a6568><!--[--><!--]--><main class="main" data-v-479a6568><div style="position:relative;" class="vp-doc _docs_other_chunking-strategies" data-v-479a6568><div><h2 id="大模型rag中的分块策略" tabindex="-1">大模型RAG中的分块策略 <a class="header-anchor" href="#大模型rag中的分块策略" aria-label="Permalink to &quot;大模型RAG中的分块策略&quot;">​</a></h2><p>分块策略在检索增强生成（RAG）方法中起着至关重要的作用，它使文档能够被划分为可管理的部分，同时保持上下文。每种方法都有其特定的优势，适用于特定的用例。</p><p>将大型数据文件拆分为更易于管理的段是提高LLM应用效率的最关键步骤之一。目标是向LLM提供完成特定任务所需的确切信息，不多也不少。</p><p>“我的解决方案中应该采用何种合适的分块策略”是LLM实践者在构建高级 RAG 解决方案时必须做出的初始和基本决策之一。</p><h3 id="关于langchat" tabindex="-1">关于LangChat <a class="header-anchor" href="#关于langchat" aria-label="Permalink to &quot;关于LangChat&quot;">​</a></h3><p><strong>LangChat</strong> 是Java生态下企业级AIGC项目解决方案，集成RBAC和AIGC大模型能力，帮助企业快速定制AI知识库、企业AI机器人。</p><p><strong>支持的AI大模型：</strong> Gitee AI / 阿里通义 / 百度千帆 / DeepSeek / 抖音豆包 / 智谱清言 / 零一万物 / 讯飞星火 / OpenAI / Gemini / Ollama / Azure / Claude 等大模型。</p><ul><li>官网地址：<a href="http://langchat.cn/" target="_blank" rel="noreferrer">http://langchat.cn/</a></li></ul><p><strong>开源地址：</strong></p><ul><li>Gitee：<a href="https://gitee.com/langchat/langchat" target="_blank" rel="noreferrer">https://gitee.com/langchat/langchat</a></li><li>Github：<a href="https://github.com/tycoding/langchat" target="_blank" rel="noreferrer">https://github.com/tycoding/langchat</a></li></ul><p><img src="http://cdn.tycoding.cn/docs/202502151026673.png" alt="iShot_2025-02-12_12.18.53" loading="lazy"></p><p><img src="https://miro.medium.com/v2/resize:fit:1400/1*VhFr2tr_FbTjzNyNv5DjWw.png" alt="img" loading="lazy"></p><h1 id="什么是分块" tabindex="-1">什么是分块？ <a class="header-anchor" href="#什么是分块" aria-label="Permalink to &quot;什么是分块？&quot;">​</a></h1><p>分块涉及将文本划分为可管理的单元或“块”，以实现高效处理。这种分割对于语义搜索、信息检索和生成式 AI 应用等任务至关重要。每个块都保留上下文和语义完整性，以确保结果连贯。</p><h1 id="_2-分块技术及其策略" tabindex="-1">2. 分块技术及其策略 <a class="header-anchor" href="#_2-分块技术及其策略" aria-label="Permalink to &quot;2. 分块技术及其策略&quot;">​</a></h1><p>各种分块技术根据文本结构和应用需求满足特定需求：</p><ul><li>固定长度分块：根据标记、单词或字符将文本分割成统一的大小。这种方法计算效率高，但可能在边界处切断有意义的上下文。</li><li>基于句子的分块：按句子分割文本，保留语法和上下文完整性。非常适合对话模型，但可能对较长的文本效率不高。</li><li>基于段落的分块：按段落分组文本，保持主题上下文。适用于结构化文档，但可能对精细调整的任务失去粒度。</li><li>语义分块：专注于按意义分组文本，而不是结构。这确保了语义连贯性，但增加了计算开销，因为它需要深入的语言理解。</li><li>滑动窗口分块：使用重叠窗口对文本进行分段，减少块边界处的信息损失。它确保更好的上下文保留，但会增加内存和处理成本。</li><li>文档分块：将整个文档视为一个单一块。这种方法对于保持整体上下文有效，但由于内存限制，可能不适用于大型文本。</li></ul><h1 id="_3-分块优化关键策略" tabindex="-1">3. 分块优化关键策略 <a class="header-anchor" href="#_3-分块优化关键策略" aria-label="Permalink to &quot;3. 分块优化关键策略&quot;">​</a></h1><p>为了最大化分块的优势，采用以下策略：</p><ul><li>重叠块：包括块之间的某些重叠可以确保在段落之间不会丢失关键信息。这对于需要无缝过渡的任务尤其重要，如对话生成或摘要。</li><li>动态块大小：根据模型的容量或文本的复杂性调整块大小可以提升性能。较小的块适合 BERT 等模型，而较大的块适用于需要更广泛上下文的生成任务。</li><li>：递归或多级分块允许处理复杂的文本结构，例如将文档拆分为章节、节和段落。</li><li>向量化的对齐：分块技术的选择对检索系统中的向量表示有显著影响。句子转换器和 BERT 或 GPT 等嵌入通常用于与分块粒度对齐的最佳向量化</li></ul><h1 id="_4-优点与局限性" tabindex="-1">4. 优点与局限性 <a class="header-anchor" href="#_4-优点与局限性" aria-label="Permalink to &quot;4. 优点与局限性&quot;">​</a></h1><ul><li><strong>好处：</strong></li><li>增强上下文理解。</li><li>支持 RAG 系统中高效的索引和检索。</li><li>保持生成模型中更好的准确性，语义连贯性。</li><li>限制：</li><li>计算上对语义和重叠分块较为昂贵。</li><li>需要调整以平衡上下文保留和处理效率。</li></ul><h1 id="_5-应用场景" tabindex="-1">5. 应用场景 <a class="header-anchor" href="#_5-应用场景" aria-label="Permalink to &quot;5. 应用场景&quot;">​</a></h1><p>分词在以下方面被广泛使用：</p><ul><li>检索系统：在搜索引擎或聊天机器人中检索回答查询的相关片段。</li><li>生成模型：保持文本生成的上下文连贯。</li><li>学术和法律研究：确保对结构化和复杂文档进行详细、有意义的分段。</li></ul><p>通过采用适当的分块策略，从业者可以提升检索和生成系统的性能，在计算资源和上下文准确性之间取得平衡。</p><h1 id="_1-固定长度分块" tabindex="-1">1. 固定长度分块 <a class="header-anchor" href="#_1-固定长度分块" aria-label="Permalink to &quot;1. 固定长度分块&quot;">​</a></h1><p>固定长度分块将文本分割成指定字符数或词数的块。这种方法简单直接，但往往存在将有意义的内容分割开的风险，导致上下文丢失。</p><p>如何工作：<strong>如何工作</strong></p><ul><li>该方法根据预定义的长度（例如，单词数、标记或字符数）将文本划分为均匀的块。</li><li>例如，一个包含 100 个单词的段落可能被分成十个 10 个单词的片段。</li></ul><p>优势：</p><ul><li>简洁性与计算效率。</li><li>适用于结构化文本或当上下文边界不是关键时。</li></ul><p>缺点：</p><ul><li>可能会在块之间分割句子或想法，导致语义连贯性丧失。</li></ul><p>示例：<strong>示例</strong>：</p><ul><li>文本分块通过将文本分解成部分来提高检索。</li><li>固定长度分块（每块 10 个字符）：</li><li>块 1：“Chunking i”</li><li>块 2：“提高 re”</li><li>块 3：“通过”检索</li></ul><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Input: &quot;The quick brown fox jumps over the lazy dog. It is a bright sunny day.&quot;</span></span>
<span class="line"><span>Chunk Size: 10 characters</span></span>
<span class="line"><span>Chunks:</span></span>
<span class="line"><span>  1. &quot;The quick &quot;</span></span>
<span class="line"><span>  2. &quot;brown fox &quot;</span></span>
<span class="line"><span>  3. &quot;jumps over&quot;</span></span>
<span class="line"><span>  4. &quot; the lazy&quot;</span></span>
<span class="line"><span>  5. &quot; dog. It&quot;</span></span>
<span class="line"><span>  6. &quot; is a bri&quot;</span></span>
<span class="line"><span>  7. &quot;ght sunny&quot;</span></span>
<span class="line"><span>  8. &quot; day.&quot;</span></span></code></pre></div><ul><li>影响：分割句子或思想会导致语义连贯性丧失。</li></ul><p>代码示例</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>def fixed_length_chunk(text, size):</span></span>
<span class="line"><span>    return [text[i:i+size] for i in range(0, len(text), size)]</span></span></code></pre></div><h1 id="_2-语义块切分" tabindex="-1">2. 语义块切分 <a class="header-anchor" href="#_2-语义块切分" aria-label="Permalink to &quot;2. 语义块切分&quot;">​</a></h1><p>文本根据语义连贯性分成块，确保每个块都是一个有意义的单元。这通常需要使用嵌入来找到逻辑边界。</p><p><img src="https://miro.medium.com/v2/resize:fit:1400/0*eZ_l7rinqtWWAlBH.png" alt="img" loading="lazy"></p><p>如何工作：<strong>如何工作</strong></p><ul><li>文本根据语义连贯性而非固定大小进行划分。</li><li>使用自然语言理解（NLP）来识别逻辑断点，如句子或主题边界。</li></ul><p>优势：</p><ul><li>保留每个片段的意义和上下文。</li><li>提升检索增强生成（RAG）任务的准确性。</li></ul><p>缺点：</p><ul><li>计算成本高，因为它需要语义解析。</li></ul><p>示例：<strong>示例</strong>：</p><ul><li>AI 研究涵盖各种主题。机器学习专注于模式。</li><li>语义块：</li><li>块 1：“人工智能研究涵盖各种主题。”</li><li>机器学习专注于模式。</li></ul><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Input: &quot;The quick brown fox jumps over the lazy dog. It is a bright sunny day.&quot;</span></span>
<span class="line"><span>Chunks:</span></span>
<span class="line"><span>  1. &quot;The quick brown fox jumps over the lazy dog.&quot;</span></span>
<span class="line"><span>  2. &quot;It is a bright sunny day.&quot;</span></span></code></pre></div><ul><li>影响：通过保留信息的逻辑流程来提高检索准确性。</li><li>代码示例</li></ul><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>from sentence_transformers import SentenceTransformer</span></span>
<span class="line"><span></span></span>
<span class="line"><span>def semantic_chunk(text, max_tokens, model):</span></span>
<span class="line"><span>    sentences = text.split(&#39;. &#39;)</span></span>
<span class="line"><span>    chunks, current_chunk = [], &quot;&quot;</span></span>
<span class="line"><span>    for sentence in sentences:</span></span>
<span class="line"><span>        if len(current_chunk) + len(sentence) &lt;= max_tokens:</span></span>
<span class="line"><span>            current_chunk += sentence + &quot;. &quot;</span></span>
<span class="line"><span>        else:</span></span>
<span class="line"><span>            chunks.append(current_chunk.strip())</span></span>
<span class="line"><span>            current_chunk = sentence + &quot;. &quot;</span></span>
<span class="line"><span>    if current_chunk:</span></span>
<span class="line"><span>        chunks.append(current_chunk.strip())</span></span>
<span class="line"><span>    return chunks</span></span></code></pre></div><h1 id="_3-递归字符分块" tabindex="-1">3. 递归字符分块 <a class="header-anchor" href="#_3-递归字符分块" aria-label="Permalink to &quot;3. 递归字符分块&quot;">​</a></h1><p>初始块基于字符限制创建。如果它们太大，则递归地分成更小的、具有语义意义的单元（例如，句子）。</p><p>如何工作：<strong>如何工作</strong></p><ul><li>最初，根据字符限制（例如，500 个字符）创建大块内容。</li><li>如果一个块超过了限制，它将被递归地分割成更小的有意义的单位，例如句子。</li></ul><p>优势：</p><ul><li>保留语义完整性，同时遵守尺寸限制。</li><li>非常适合存在 API 限制的情况（例如，OpenAI 的令牌限制）。</li></ul><p>缺点：</p><ul><li>递归处理会增加计算时间。</li></ul><p>示例：<strong>示例</strong>：</p><ul><li>文本分块将文本分解成更小的部分。此方法增强了检索。</li><li>递归字符限制（20 个字符）：</li><li>文本块 1：“分块处理文本”</li><li>块 2：“分成更小的部分。”</li><li>块 3：“此方法增强”</li><li>块 4：“检索。”</li></ul><p>示例：<strong>示例</strong>：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Input: &quot;The quick brown fox jumps over the lazy dog. It is a bright sunny day.&quot;</span></span>
<span class="line"><span>Step 1: Chunk by character size (50).</span></span>
<span class="line"><span>Step 2: Further divide large chunks into sentences.</span></span>
<span class="line"><span>Chunks:</span></span>
<span class="line"><span>  1. &quot;The quick brown fox jumps over the lazy dog.&quot;</span></span>
<span class="line"><span>  2. &quot;It is a bright sunny day.&quot;</span></span></code></pre></div><p>影响：平衡块大小和连贯性。</p><p>代码示例</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>def recursive_chunk(text, char_limit):</span></span>
<span class="line"><span>    if len(text) &lt;= char_limit:</span></span>
<span class="line"><span>        return [text]</span></span>
<span class="line"><span>    midpoint = len(text) // 2</span></span>
<span class="line"><span>    for i in range(midpoint, len(text)):</span></span>
<span class="line"><span>        if text[i] in &#39;.!?&#39;:</span></span>
<span class="line"><span>            return [text[:i+1].strip()] + recursive_chunk(text[i+1:].strip(), char_limit)</span></span></code></pre></div><h1 id="_4-自适应分块" tabindex="-1">4. 自适应分块 <a class="header-anchor" href="#_4-自适应分块" aria-label="Permalink to &quot;4. 自适应分块&quot;">​</a></h1><p>动态调整块大小，根据内容的复杂度或重要性，利用自然语言处理技术识别逻辑终点。</p><p>如何工作：<strong>如何工作</strong></p><ul><li>动态调整块大小，基于内容复杂度。</li><li>使用先进的自然语言处理技术来查找逻辑端点。</li></ul><p>优势：</p><ul><li>平衡计算效率和语义连贯性。</li><li>有效处理复杂和可变长度的内容。</li></ul><p>缺点：</p><ul><li>实施复杂性。</li></ul><p>示例：<strong>示例</strong>：</p><ul><li>“简单想法适合小块。复杂概念需要更大的块。”</li><li>自适应块</li><li>块 1：“简单想法适合小块。”</li><li>块 2：“复杂概念需要更大的块。”</li></ul><p>示例：<strong>示例</strong>：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Input: &quot;The quick brown fox jumps over the lazy dog. It is a bright sunny day.&quot;</span></span>
<span class="line"><span>Output:</span></span>
<span class="line"><span>  1. &quot;The quick brown fox jumps over the lazy dog.&quot;</span></span>
<span class="line"><span>  2. &quot;It is a bright sunny day.&quot;</span></span></code></pre></div><p>影响：适应不同类型的文档，提高混合内容情况下的性能。</p><p><strong>代码示例</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>def adaptive_chunk(text, nlp_model):</span></span>
<span class="line"><span>    doc = nlp_model(text)</span></span>
<span class="line"><span>    chunks = []</span></span>
<span class="line"><span>    for sent in doc.sents:</span></span>
<span class="line"><span>        chunks.append(sent.text.strip())</span></span>
<span class="line"><span>    return chunks</span></span></code></pre></div><h1 id="_5-混合分块" tabindex="-1">5. 混合分块 <a class="header-anchor" href="#_5-混合分块" aria-label="Permalink to &quot;5. 混合分块&quot;">​</a></h1><ul><li>解释：结合固定长度和语义分块，允许在块大小上保持灵活性，同时保持语义连贯性。</li></ul><p>如何工作：<strong>如何工作</strong></p><ul><li>结合固定大小和语义分块策略。</li><li>允许在保持上下文的同时调整块大小。</li></ul><p>优势：</p><ul><li>可定制以适应特定应用。</li><li>平衡精度与效率。</li></ul><p>缺点：</p><ul><li>需要仔细调整以避免冗余或失真。</li></ul><p>示例：<strong>示例</strong>：</p><ul><li>“分块提高检索效率。灵活性对于精确度至关重要。”</li><li>混合块：</li><li>块 1：“分块提高检索。”</li><li>块 2：“灵活性对精度至关重要。”</li></ul><p>示例：<strong>示例</strong>：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Input: &quot;The quick brown fox jumps over the lazy dog. It is a bright sunny day.&quot;</span></span>
<span class="line"><span>Chunk Size: 10 words</span></span>
<span class="line"><span>Chunks:</span></span>
<span class="line"><span>  1. &quot;The quick brown fox jumps over the lazy dog.&quot;</span></span>
<span class="line"><span>  2. &quot;It is a bright sunny day.&quot;</span></span></code></pre></div><ul><li>影响：针对既需要上下文又需要计算效率的系统进行了优化。</li></ul><p>代码示例</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>def hybrid_chunk(text, word_limit, semantic=True):</span></span>
<span class="line"><span>    words = text.split()</span></span>
<span class="line"><span>    chunks = []</span></span>
<span class="line"><span>    for i in range(0, len(words), word_limit):</span></span>
<span class="line"><span>        chunk = &quot; &quot;.join(words[i:i+word_limit])</span></span>
<span class="line"><span>        if semantic and chunk[-1] not in &#39;.!?&#39;:</span></span>
<span class="line"><span>            chunk += &#39;.&#39;</span></span>
<span class="line"><span>        chunks.append(chunk.strip())</span></span>
<span class="line"><span>    return chunks</span></span></code></pre></div><h1 id="_6-重叠分块" tabindex="-1">6. 重叠分块 <a class="header-anchor" href="#_6-重叠分块" aria-label="Permalink to &quot;6. 重叠分块&quot;">​</a></h1><p>块重叠一定间距，确保边界处不丢失上下文。</p><p>如何工作：<strong>如何工作</strong></p><ul><li>创建重叠块以保留边界之间的上下文。</li><li>确保一个块中的关键思想不会在块之间丢失。</li></ul><p>优势：</p><ul><li>保留更多上下文。</li><li>提升检索准确性。</li></ul><p>缺点：</p><ul><li>冗余增加内存使用。</li></ul><p>示例：<strong>示例</strong>：</p><ul><li>文本分块通过将文本分解成部分来提高检索。</li><li>重叠块（5 词重叠）：</li><li>块 1：“分块通过分割提高检索”</li><li>块 2：“通过将文本拆分来检索”</li><li>块 3：“将文本拆分成部分。”</li></ul><p>示例：<strong>示例</strong>：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Input: &quot;The quick brown fox jumps over the lazy dog. It is a bright sunny day.&quot;</span></span>
<span class="line"><span>Overlap: 5 words</span></span>
<span class="line"><span>Chunks:</span></span>
<span class="line"><span>  1. &quot;The quick brown fox jumps over the lazy&quot;</span></span>
<span class="line"><span>  2. &quot;fox jumps over the lazy dog. It is a&quot;</span></span>
<span class="line"><span>  3. &quot;dog. It is a bright sunny day.&quot;</span></span></code></pre></div><ul><li>影响：跨边界保持上下文，提高检索质量。</li><li>代码示例</li></ul><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>def overlapping_chunk(text, size, overlap):</span></span>
<span class="line"><span>    words = text.split()</span></span>
<span class="line"><span>    chunks = []</span></span>
<span class="line"><span>    for i in range(0, len(words), size - overlap):</span></span>
<span class="line"><span>        chunks.append(&quot; &quot;.join(words[i:i+size]))</span></span>
<span class="line"><span>    return chunks</span></span></code></pre></div><h1 id="_7-字符文本分割" tabindex="-1">7. 字符文本分割 <a class="header-anchor" href="#_7-字符文本分割" aria-label="Permalink to &quot;7. 字符文本分割&quot;">​</a></h1><p>如何工作：<strong>如何工作</strong></p><ul><li>根据特定的字符限制分割文本。</li></ul><p>优势：</p><ul><li>快速且直接。</li><li>适用于具有令牌或字符限制的系统。</li></ul><p>缺点：</p><ul><li>风险在于字符在单词或句子中间掉落时破坏意义。</li></ul><h1 id="_8-使用-langchain-进行自动文本分割" tabindex="-1">8. 使用 LangChain 进行自动文本分割 <a class="header-anchor" href="#_8-使用-langchain-进行自动文本分割" aria-label="Permalink to &quot;8. 使用 LangChain 进行自动文本分割&quot;">​</a></h1><p>如何工作：<strong>如何工作</strong></p><ul><li>使用 LangChain 内置的<code>TextSplitter</code>类来自动化分块。</li><li>根据文本类型和内容结构进行适配。</li></ul><p>优势：</p><ul><li>简化处理流程。</li><li>支持各种分块配置。</li></ul><h1 id="_9-递归字符文本分割" tabindex="-1">9. 递归字符文本分割 <a class="header-anchor" href="#_9-递归字符文本分割" aria-label="Permalink to &quot;9. 递归字符文本分割&quot;">​</a></h1><p>如何工作：<strong>如何工作</strong></p><ul><li>递归方法类似于递归字符分块，但专为分词文本设计。</li></ul><h1 id="_10-文档特定拆分" tabindex="-1">10. 文档特定拆分 <a class="header-anchor" href="#_10-文档特定拆分" aria-label="Permalink to &quot;10. 文档特定拆分&quot;">​</a></h1><p>如何工作：<strong>如何工作</strong></p><ul><li>利用特定领域的拆分器（例如，<code>MarkdownSplitter</code>、<code>PythonCodeTextSplitter</code>）来处理专业文档。</li></ul><p>优势：</p><ul><li>自定义各种格式的处理。</li><li>增强特定领域检索。</li></ul><h1 id="_11-基于嵌入的语义分块" tabindex="-1">11. 基于嵌入的语义分块 <a class="header-anchor" href="#_11-基于嵌入的语义分块" aria-label="Permalink to &quot;11. 基于嵌入的语义分块&quot;">​</a></h1><p>如何工作：<strong>如何工作</strong></p><ul><li>使用语义嵌入将文本划分为连贯的片段。</li><li>将块与向量表示对齐以增强搜索。</li></ul><h1 id="_12-代理分块" tabindex="-1">12. 代理分块 <a class="header-anchor" href="#_12-代理分块" aria-label="Permalink to &quot;12. 代理分块&quot;">​</a></h1><p>代理分块关注逻辑命题或连贯的组群，将每个分块分解成有意义的行或组。</p><h2 id="基于命题的词块化" tabindex="-1">基于命题的词块化 <a class="header-anchor" href="#基于命题的词块化" aria-label="Permalink to &quot;基于命题的词块化&quot;">​</a></h2><ul><li>解释：每个块代表一个逻辑命题，独立存在，具有完整的意义。</li><li>示例：<strong>示例</strong>：</li></ul><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Input: &quot;The quick brown fox jumps. The lazy dog sleeps.&quot;</span></span>
<span class="line"><span>Chunks:</span></span>
<span class="line"><span>  1. &quot;The quick brown fox jumps.&quot;</span></span>
<span class="line"><span>  2. &quot;The lazy dog sleeps.&quot;</span></span></code></pre></div><p>影响：适用于结构化和基于规则的文本。</p><h2 id="基于分组的分块" tabindex="-1">基于分组的分块 <a class="header-anchor" href="#基于分组的分块" aria-label="Permalink to &quot;基于分组的分块&quot;">​</a></h2><ul><li>解释：根据代理驱动的启发式方法将相关块组合成连贯的单元。</li><li>代码示例</li></ul><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>def agentic_chunk(text):</span></span>
<span class="line"><span>    lines = text.split(&#39;. &#39;)</span></span>
<span class="line"><span>    chunks = [line.strip() + &#39;.&#39; for line in lines if line]</span></span>
<span class="line"><span>    return chunks</span></span></code></pre></div><h1 id="代理分块-详细解释" tabindex="-1">代理分块：详细解释 <a class="header-anchor" href="#代理分块-详细解释" aria-label="Permalink to &quot;代理分块：详细解释&quot;">​</a></h1><p><img src="https://miro.medium.com/v2/resize:fit:1400/0*eYoAJxEDa5uUKVu2.png" alt="img" loading="lazy"></p><p>代理分块是一种复杂的文本分割策略，旨在确保文本块保持其语义连贯性并传达有意义的信息。它采用两种主要子策略：</p><p><img src="https://miro.medium.com/v2/resize:fit:1400/1*2r7xsoPZx6alNY0t5kH9QQ.png" alt="img" loading="lazy"></p><h1 id="基于命题的词块化-1" tabindex="-1">基于命题的词块化 <a class="header-anchor" href="#基于命题的词块化-1" aria-label="Permalink to &quot;基于命题的词块化&quot;">​</a></h1><p>定义 基于命题的切分关注将文本分割成独立的块，其中每个部分代表一个命题或完整的想法。命题通常是一个句子或句子的一部分，传达一个完整的思想或陈述。</p><p>工作策略</p><p>命题识别</p><ul><li>使用自然语言处理（NLP）技术，对文本进行分析以识别逻辑或语法命题。</li><li>例如，由连词或标点符号连接的从句可能被分成不同的命题。</li></ul><p>分割过程</p><ul><li>每个块都是为了确保它形成一个完整且独立的陈述，即使在孤立的情况下也能保留其意义。</li></ul><p>语义完整性</p><ul><li>该过程避免了以留下不完整或模糊信息的方式分割</li></ul><p>示例：<strong>示例</strong>：</p><ul><li>人工智能（AI）正在迅速发展，它正在改变着行业。</li><li>基于命题的词块</li></ul><ol><li>人工智能（AI）正在迅速发展。</li><li>它正在改变行业。</li></ol><p>应用</p><ul><li>在需要精确事实检索的系统中有用，例如知识库、问答系统和学术文本处理。</li><li>增强信息检索的粒度，确保每个片段提供完整的知识。</li></ul><h1 id="_2-使用代理分块器进行分组分块" tabindex="-1">2. 使用代理分块器进行分组分块 <a class="header-anchor" href="#_2-使用代理分块器进行分组分块" aria-label="Permalink to &quot;2. 使用代理分块器进行分组分块&quot;">​</a></h1><p>定义 将信息分组成簇，通过将命题或句子分组到逻辑上、语义上一致的单位。目标是保持更广泛的上下文，同时将文本划分为可管理的块。</p><p>工作策略</p><p>识别关系</p><p>自然语言处理技术，如语义相似度和主题建模，被用于检测命题或句子之间的关系。</p><p>块形成：</p><p>相关命题被组合成一个单独的块。这可能是基于共享的主题、引用或逻辑流程。</p><p>尺寸优化</p><p>确保分组块不超过预定义的大小限制（例如，语言模型中的标记或字符限制）。</p><p>示例：<strong>示例</strong>：</p><ul><li>“人工智能正在迅速发展。它正在改变医疗保健和金融等行业。机器学习是人工智能的关键组成部分。”</li><li>分组块</li><li>块 1：“人工智能正在迅速发展。它正在改变医疗保健和金融等行业。”</li><li>机器学习是人工智能的关键组成部分。</li></ul><p>应用</p><ul><li>用于检索增强生成（RAG）系统，以生成保持更广泛上下文的具有意义的响应。</li><li>非常适合文档摘要，其中块之间的主题一致性至关重要。</li></ul><h1 id="代理分块的好处" tabindex="-1">代理分块的好处 <a class="header-anchor" href="#代理分块的好处" aria-label="Permalink to &quot;代理分块的好处&quot;">​</a></h1><p>语义一致性</p><ul><li>确保每个片段都包含一个有意义的独立观点或语义上连贯的观点组。</li></ul><p>改进检索准确性</p><ul><li>通过关注逻辑单元，检索系统可以更好地与用户查询对齐。</li></ul><p>上下文保留：</p><ul><li>将块分组保留了多行推理任务所需的上下文。</li></ul><p>灵活性</p><ul><li>代理分块适应性强，适用于各种用例，从细粒度事实核查到更广泛的主题摘要。</li></ul><h1 id="挑战" tabindex="-1">挑战 <a class="header-anchor" href="#挑战" aria-label="Permalink to &quot;挑战&quot;">​</a></h1><p>计算开销</p><ul><li>该过程涉及语义分析和自然语言处理计算，可能需要大量资源。</li></ul><p>《块大小调整》</p><ul><li>在粒度与上下文保留之间取得平衡需要仔细调整。</li></ul><h1 id="代码演示" tabindex="-1">代码演示 <a class="header-anchor" href="#代码演示" aria-label="Permalink to &quot;代码演示&quot;">​</a></h1><h2 id="设置环境" tabindex="-1">设置环境 <a class="header-anchor" href="#设置环境" aria-label="Permalink to &quot;设置环境&quot;">​</a></h2><p>所需库使用以下命令安装：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>pip install -U chromadb langchain llama-index langchain_experimental langchain_openai</span></span></code></pre></div><p>关键模块如用于美观打印的<code>rich</code>、用于文档和模型管理的<code>langchain</code>以及用于集成的<code>langchain_community</code>已被导入。使用 Mistral 模型初始化了本地LLM实例的<code>ChatOllama</code>类。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>local_llm = ChatOllama(model=&quot;mistral&quot;)</span></span></code></pre></div><h2 id="_2-rag-实现" tabindex="-1">2. RAG 实现 <a class="header-anchor" href="#_2-rag-实现" aria-label="Permalink to &quot;2. RAG 实现&quot;">​</a></h2><p>RAG 函数使用一个向量存储（<code>色度</code>）和基于检索的提示策略。</p><ol><li>嵌入式初始化：<code>OllamaEmbeddings</code>模型生成文本嵌入，以实现高效的存储和检索。</li><li>检索器设置：向量存储作为检索器，根据查询搜索相关片段。</li><li>提示模板：一个自定义模板为LLM生成相关答案的问题和上下文进行框架。</li></ol><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>def rag(chunks, collection_name):</span></span>
<span class="line"><span>    vectorstore = Chroma.from_documents(</span></span>
<span class="line"><span>        documents=documents,</span></span>
<span class="line"><span>        collection_name=collection_name,</span></span>
<span class="line"><span>        embedding=embeddings.ollama.OllamaEmbeddings(model=&#39;nomic-embed-text&#39;),</span></span>
<span class="line"><span>    )</span></span>
<span class="line"><span>    retriever = vectorstore.as_retriever()</span></span>
<span class="line"><span>    ...</span></span>
<span class="line"><span>    chain.invoke(&quot;What is the use of Text Splitting?&quot;)</span></span></code></pre></div><h2 id="_3-文本分割技术" tabindex="-1">3. 文本分割技术 <a class="header-anchor" href="#_3-文本分割技术" aria-label="Permalink to &quot;3. 文本分割技术&quot;">​</a></h2><p><strong>字符文本分割</strong></p><p>文本手动分割成 35 个字符大小的块。或者，<code>CharacterTextSplitter</code> 通过可选参数如 <code>chunk_size</code>、<code>chunk_overlap</code> 和 <code>separator</code> 自动化此过程。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>text_splitter = CharacterTextSplitter(chunk_size=35, chunk_overlap=0)</span></span>
<span class="line"><span>documents = text_splitter.create_documents([text])</span></span></code></pre></div><p><strong>递归字符文本分割</strong></p><p>这种方法智能地使用多个分隔符（例如，换行符、空格）来分割文本，以保持语义连贯。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>text_splitter = RecursiveCharacterTextSplitter(chunk_size=65, chunk_overlap=0)</span></span>
<span class="line"><span>documents = text_splitter.create_documents([text])</span></span></code></pre></div><h2 id="_4-文档特定拆分" tabindex="-1">4. 文档特定拆分 <a class="header-anchor" href="#_4-文档特定拆分" aria-label="Permalink to &quot;4. 文档特定拆分&quot;">​</a></h2><p><strong>Markdown 分割</strong></p><p>Markdown 特定的分割确保基于结构标记（如标题或列表）的逻辑分组。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>splitter = MarkdownTextSplitter(chunk_size=40, chunk_overlap=0)</span></span>
<span class="line"><span>documents = splitter.create_documents([markdown_text])</span></span></code></pre></div><p><strong>代码拆分</strong></p><p>编程语言如 Python 和 JavaScript 根据语法进行划分，以保持功能性和可读性。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>python_splitter = PythonCodeTextSplitter(chunk_size=100)</span></span>
<span class="line"><span>documents = python_splitter.create_documents([python_text])</span></span></code></pre></div><h2 id="_5-高级语义分块" tabindex="-1">5. 高级语义分块 <a class="header-anchor" href="#_5-高级语义分块" aria-label="Permalink to &quot;5. 高级语义分块&quot;">​</a></h2><p>语义分块使用嵌入来计算句子之间的语义相似度，并根据预定义的阈值（例如，百分位数）进行分割。这确保了块在意义上被有意义地分隔。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>from langchain_experimental.text_splitter import SemanticChunker</span></span>
<span class="line"><span>text_splitter = SemanticChunker(OpenAIEmbeddings(), breakpoint_threshold_type=&quot;percentile&quot;)</span></span>
<span class="line"><span>documents = text_splitter.create_documents([text])</span></span></code></pre></div><h2 id="_6-代理分块" tabindex="-1">6. 代理分块 <a class="header-anchor" href="#_6-代理分块" aria-label="Permalink to &quot;6. 代理分块&quot;">​</a></h2><p>代理分块引入了基于命题的分块方法，其中句子或命题通过基于LLM的处理流程提取。这些命题使用<code>AgenticChunker</code>逻辑分组。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>from agentic_chunker import AgenticChunker</span></span>
<span class="line"><span>ac = AgenticChunker()</span></span>
<span class="line"><span>ac.add_propositions(text_propositions)</span></span>
<span class="line"><span>chunks = ac.get_chunks(get_type=&#39;list_of_strings&#39;)</span></span>
<span class="line"><span>documents = [Document(page_content=chunk, metadata={&quot;source&quot;: &quot;local&quot;}) for chunk in chunks]</span></span></code></pre></div><h1 id="执行" tabindex="-1">执行 <a class="header-anchor" href="#执行" aria-label="Permalink to &quot;执行&quot;">​</a></h1><p>代码最终将上述所有策略集成到一个 RAG 管道中。文本分割准备数据，而 RAG 函数使基于分割文档的问答变得高效。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>rag(documents, &quot;agentic-chunks&quot;)</span></span></code></pre></div><h1 id="实施优势" tabindex="-1">实施优势 <a class="header-anchor" href="#实施优势" aria-label="Permalink to &quot;实施优势&quot;">​</a></h1><ol><li>可扩展性：LangChain 的文本拆分技术的模块化支持各种文档格式和用例。</li><li>精度：先进的分割策略，如语义和代理分块，确保高质量的分段。</li><li>与 RAG 集成：直接集成到基于检索的工作流程中，以增强性能。</li></ol><p>参考：<a href="https://medium.com/@danushidk507/chunking-strategies-f93dbdec7634" target="_blank" rel="noreferrer">https://medium.com/@danushidk507/chunking-strategies-f93dbdec7634</a></p><h2 id="联系我" tabindex="-1">联系我 <a class="header-anchor" href="#联系我" aria-label="Permalink to &quot;联系我&quot;">​</a></h2><p>最后，推荐大家关注一下开源项目：LangChat，Java生态下的AIGC大模型产品解决方案。</p><ul><li>LangChat产品官网：<a href="https://langchat.cn/" target="_blank" rel="noreferrer">https://langchat.cn/</a></li><li>Github: <a href="https://github.com/TyCoding/langchat" target="_blank" rel="noreferrer">https://github.com/TyCoding/langchat</a></li><li>Gitee: <a href="https://gitee.com/langchat/langchat" target="_blank" rel="noreferrer">https://gitee.com/langchat/langchat</a></li><li>微信：LangchainChat</li></ul><p><img src="http://cdn.tycoding.cn/docs/202502151026673.png" alt="" loading="lazy"></p></div></div></main><footer class="VPDocFooter" data-v-479a6568 data-v-a9f44413><!--[--><!--]--><div class="edit-info" data-v-a9f44413><!----><div class="last-updated" data-v-a9f44413><p class="VPLastUpdated" data-v-a9f44413 data-v-0500d987>Last updated: <time datetime="2025-02-26T01:05:02.000Z" data-v-0500d987></time></p></div></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-a9f44413><span class="visually-hidden" id="doc-footer-aria-label" data-v-a9f44413>Pager</span><div class="pager" data-v-a9f44413><a class="VPLink link pager-link prev" href="/docs/deploy/deploy.html" data-v-a9f44413><!--[--><span class="desc" data-v-a9f44413>Previous page</span><span class="title" data-v-a9f44413>LangChat部署教程</span><!--]--></a></div><div class="pager" data-v-a9f44413><a class="VPLink link pager-link next" href="/docs/other/claude-3-7-sonnet.html" data-v-a9f44413><!--[--><span class="desc" data-v-a9f44413>Next page</span><span class="title" data-v-a9f44413>Claude 3.7 Sonnet强势来袭</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"docs_deploy_deploy.md\":\"Dg2bS27a\",\"docs_exercise_langchat-deepseek-r1.md\":\"BG1HS0Jq\",\"docs_exercise_oss-minio.md\":\"BKGFqqWT\",\"docs_exercise_rag.md\":\"YedSmvND\",\"docs_other_bank.md\":\"5tiAkFLT\",\"docs_other_chunking-strategies.md\":\"gV_LewAr\",\"docs_other_claude-3-7-sonnet.md\":\"Dj1Hhibl\",\"docs_other_deepseek-r1-architecture-and-training.md\":\"0Z6H9Kbq\",\"docs_other_deepseek-r1-distilled-models.md\":\"4nK05JS_\",\"docs_other_deepseek-r1-reasoning-capabilities-analysis.md\":\"B5CK-D6e\",\"docs_other_deepseek-r1-tuning.md\":\"CdRsqumj\",\"docs_other_distill-deepseek-r1-into-your-model.md\":\"DdEwVv1R\",\"docs_other_guide-getting-started-with-cursor-and-deepseek-r1.md\":\"CrF2GZkG\",\"docs_other_hardware-guide-for-llm-training-and-fine-tuning.md\":\"CyLRPjt8\",\"docs_other_markitdown-a-deep-dive.md\":\"Cs6UPOgy\",\"docs_other_reasoning-modes-vs-other-ai-models.md\":\"BmYH1TUj\",\"docs_other_the-ai-web-search-landscape.md\":\"C4xGfer5\",\"docs_other_top11-ai-chat-ui-for-developers.md\":\"CSnixW_I\",\"docs_other_top8-on-premise-plans-for-deepseek-r1.md\":\"BroA2pse\",\"docs_other_vllm-llm-local-inference-library.md\":\"7uM8sIzs\",\"docs_other_vllm-ollama-comprehensive-comparison.md\":\"D9IVeAkT\",\"docs_other_vllm-vs-ollama.md\":\"BRFYvY27\",\"docs_start_environment.md\":\"DkSgePUm\",\"docs_start_getting-started.md\":\"B8WJQYY8\",\"docs_start_introduce.md\":\"CgbmexuX\",\"docs_start_knowledge.md\":\"Ce41O1tK\",\"docs_start_login.md\":\"DWOURVt-\",\"docs_start_models-proxy.md\":\"6b6woVJN\",\"docs_start_models.md\":\"Ds4X4F2M\",\"docs_start_questions.md\":\"AMWgx7AD\",\"index.md\":\"n9TEBaM7\",\"readme.md\":\"DKy5OORU\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"LangChat Docs\",\"description\":\"LangChat Project Document\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"outline\":{\"level\":\"deep\"},\"search\":{\"provider\":\"local\"},\"nav\":[{\"text\":\"LangChat\",\"link\":\"/\"},{\"text\":\"LangChat文档\",\"link\":\"/docs/exercise/langchat-deepseek-r1\",\"activeMatch\":\"/docs\"},{\"text\":\"在线预览\",\"items\":[{\"text\":\"LangChat官网\",\"link\":\"https://langchat.cn/\"},{\"text\":\"LangChat后台预览\",\"link\":\"http://backend.langchat.cn/\"},{\"text\":\"LangChat LLM Ops\",\"link\":\"http://llm.langchat.cn\"},{\"text\":\"LangChat UPMS Ops\",\"link\":\"http://upms.langchat.cn\"}]}],\"sidebar\":{\"/docs\":[{\"text\":\"LangChat实战\",\"items\":[{\"text\":\"DeepSeek-R1实战\",\"link\":\"/docs/exercise/langchat-deepseek-r1\"},{\"text\":\"使用Minio作为OSS\",\"link\":\"/docs/exercise/oss-minio\"},{\"text\":\"LLM-RAG基础概念\",\"link\":\"/docs/exercise/rag\"}]},{\"text\":\"LangChat配置\",\"items\":[{\"text\":\"LangChat介绍\",\"link\":\"/docs/start/introduce\"},{\"text\":\"环境准备\",\"link\":\"/docs/start/environment\"},{\"text\":\"快速开始\",\"link\":\"/docs/start/getting-started\"},{\"text\":\"登录LangChat\",\"link\":\"/docs/start/login\"},{\"text\":\"模型配置\",\"link\":\"/docs/start/models\"},{\"text\":\"模型代理\",\"link\":\"/docs/start/models-proxy\"},{\"text\":\"知识库\",\"link\":\"/docs/start/knowledge\"},{\"text\":\"常见问题\",\"link\":\"/docs/start/questions\"}]},{\"text\":\"LangChat部署\",\"items\":[{\"text\":\"LangChat部署教程\",\"link\":\"/docs/deploy/deploy\"}]},{\"text\":\"推荐阅读\",\"items\":[{\"text\":\"大模型RAG中的分块策略\",\"link\":\"/docs/other/chunking-strategies\"},{\"text\":\"Claude 3.7 Sonnet强势来袭\",\"link\":\"/docs/other/claude-3-7-sonnet\"},{\"text\":\"DeepSeek R1架构和训练过程图解\",\"link\":\"/docs/other/deepseek-r1-architecture-and-training\"},{\"text\":\"DeepSeek-R1蒸馏模型\",\"link\":\"/docs/other/deepseek-r1-distilled-models\"},{\"text\":\"DeepSeek-R1的推理能力分析\",\"link\":\"/docs/other/deepseek-r1-reasoning-capabilities-analysis\"},{\"text\":\"DeepSeek-R1微调指南\",\"link\":\"/docs/other/deepseek-r1-tuning\"},{\"text\":\"蒸馏DeepSeek-R1到自己的模型\",\"link\":\"/docs/other/distill-deepseek-r1-into-your-model\"},{\"text\":\"Cursor + DeepSeek R1 使用指南\",\"link\":\"/docs/other/guide-getting-started-with-cursor-and-deepseek-r1\"},{\"text\":\"大模型训练/微调硬件指南\",\"link\":\"/docs/other/hardware-guide-for-llm-training-and-fine-tuning\"},{\"text\":\"MarkItDown深入研究\",\"link\":\"/docs/other/markitdown-a-deep-dive\"},{\"text\":\"推理模型 vs. 其他AI模型\",\"link\":\"/docs/other/reasoning-modes-vs-other-ai-models\"},{\"text\":\"AI搜索引擎生态全景\",\"link\":\"/docs/other/the-ai-web-search-landscape\"},{\"text\":\"8个DeepSeek-R1私有化部署方案\",\"link\":\"/docs/other/top8-on-premise-plans-for-deepseek-r1\"},{\"text\":\"11个开发人员必备AI聊天界面\",\"link\":\"/docs/other/top11-ai-chat-ui-for-developers\"},{\"text\":\"vLLM 大模型本地推理库\",\"link\":\"/docs/other/vllm-llm-local-inference-library\"},{\"text\":\"vLLM/ollama综合对比\",\"link\":\"/docs/other/vllm-ollama-comprehensive-comparison\"},{\"text\":\"VLLM vs. Ollama\",\"link\":\"/docs/other/vllm-vs-ollama\"}]}]},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/TyCoding/langchat\"}]},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>